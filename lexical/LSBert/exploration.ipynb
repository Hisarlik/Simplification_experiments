{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replicated from : https://medium.com/@armandj.olivares/how-to-use-bert-for-lexical-simplification-6edbf5a4d15e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "from functools import lru_cache\n",
    "from collections import Counter\n",
    "from collections import namedtuple\n",
    "\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "from nltk import word_tokenize\n",
    "#nltk.download('brown')\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.callbacks\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from wordfreq import zipf_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset = namedtuple('Dataset', 'name, train, test')\n",
    "ModelTuple = namedtuple('Model', 'type, name, dimension, corpus, model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas config\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_colwidth', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAIN_PATH_DATASET = \"../../data/cwi_shared_dataset/traindevset/english/\"\n",
    "MAIN_PATH_EMBEDDINGS= '../../data/glove.6B/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "genres = ['Wikipedia', 'WikiNews', 'News']\n",
    "datasets = ['Train', 'Dev']\n",
    "columns = ['id', 'sentence', \"start\", \"end\", \"target\", \n",
    "           \"nat\", \"non_nat\", \"nat_marked\", \"non_nat_marked\", \"binary\", \"prob\"]\n",
    "\n",
    "\n",
    "datasets = [Dataset('Wikipedia', 'Train', 'Dev'),\n",
    "            Dataset('WikiNews', 'Train', 'Dev'),\n",
    "            Dataset('News', 'Train', 'Dev')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_df(path):\n",
    "    df = pd.read_csv(path, header=None, sep = \"\\t\")\n",
    "    df.columns = columns\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [Dataset(d.name, load_df(MAIN_PATH_DATASET + d.name + '_' + d.train + '.tsv'),\n",
    "                            load_df(MAIN_PATH_DATASET + d.name + '_' + d.test + '.tsv'))\n",
    "                            for d in datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>target</th>\n",
       "      <th>nat</th>\n",
       "      <th>non_nat</th>\n",
       "      <th>nat_marked</th>\n",
       "      <th>non_nat_marked</th>\n",
       "      <th>binary</th>\n",
       "      <th>prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>Normally , the land will be passed down to future generations in a way that recognizes the community 's traditional connection to that country .</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>Normally</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>Normally , the land will be passed down to future generations in a way that recognizes the community 's traditional connection to that country .</td>\n",
       "      <td>28</td>\n",
       "      <td>34</td>\n",
       "      <td>passed</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>Normally , the land will be passed down to future generations in a way that recognizes the community 's traditional connection to that country .</td>\n",
       "      <td>15</td>\n",
       "      <td>19</td>\n",
       "      <td>land</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>Normally , the land will be passed down to future generations in a way that recognizes the community 's traditional connection to that country .</td>\n",
       "      <td>43</td>\n",
       "      <td>49</td>\n",
       "      <td>future</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>Normally , the land will be passed down to future generations in a way that recognizes the community 's traditional connection to that country .</td>\n",
       "      <td>43</td>\n",
       "      <td>61</td>\n",
       "      <td>future generations</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               id  \\\n",
       "0  3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "1  3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "2  3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "3  3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "4  3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "\n",
       "                                                                                                                                           sentence  \\\n",
       "0  Normally , the land will be passed down to future generations in a way that recognizes the community 's traditional connection to that country .   \n",
       "1  Normally , the land will be passed down to future generations in a way that recognizes the community 's traditional connection to that country .   \n",
       "2  Normally , the land will be passed down to future generations in a way that recognizes the community 's traditional connection to that country .   \n",
       "3  Normally , the land will be passed down to future generations in a way that recognizes the community 's traditional connection to that country .   \n",
       "4  Normally , the land will be passed down to future generations in a way that recognizes the community 's traditional connection to that country .   \n",
       "\n",
       "   start  end              target  nat  non_nat  nat_marked  non_nat_marked  \\\n",
       "0      0    8            Normally   10       10           0               1   \n",
       "1     28   34              passed   10       10           0               1   \n",
       "2     15   19                land   10       10           0               0   \n",
       "3     43   49              future   10       10           1               0   \n",
       "4     43   61  future generations   10       10           1               2   \n",
       "\n",
       "   binary  prob  \n",
       "0       1  0.05  \n",
       "1       1  0.05  \n",
       "2       0  0.00  \n",
       "3       1  0.05  \n",
       "4       1  0.15  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[0].train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Antonio\\AppData\\Local\\Temp/ipykernel_18960/1946840910.py:9: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n",
      "  glove2word2vec(glove_file, tmp_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model : glove.6B.300d.txt\n",
      "[Model(type='glove', name='glove.6B.300d.txt', dimension=300, corpus='wikipedia+gigaword5', model=<gensim.models.keyedvectors.KeyedVectors object at 0x000001F77466FE50>)]\n"
     ]
    }
   ],
   "source": [
    "# load embedding model\n",
    "glove_models = []\n",
    "\n",
    "glove_defs = [ ModelTuple('glove', 'glove.6B.300d.txt', 300, 'wikipedia+gigaword5', None)]\n",
    "              \n",
    "for model in glove_defs:\n",
    "    glove_file = MAIN_PATH_EMBEDDINGS + model.name\n",
    "    tmp_file = get_tmpfile(model.name + '-temp')\n",
    "    glove2word2vec(glove_file, tmp_file)\n",
    "    vecs = KeyedVectors.load_word2vec_format(tmp_file)\n",
    "    glove_models.append(ModelTuple(model.type, model.name, model.dimension, model.corpus, vecs))\n",
    "    print('load model : {}'.format(model.name))\n",
    "    \n",
    "print(glove_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49815\n"
     ]
    }
   ],
   "source": [
    "wordlist_lowercased = set(i.lower() for i in brown.words())\n",
    "print (len(wordlist_lowercased))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets.append(Dataset('train_all_test_wiki', \n",
    "        datasets[0].train.append(datasets[1].train).append(datasets[2].train), datasets[0].test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Rows train : 27299\n",
      "# Rows test : 694\n",
      "# Rows dataset : 27993\n",
      "# Sents train : 1988\n",
      "# Sents test : 53\n",
      "# Sents dataset : 2041\n"
     ]
    }
   ],
   "source": [
    "# Append train and test set\n",
    "dataset_sel = datasets[3]\n",
    "train_num_rows = dataset_sel.train.shape[0]\n",
    "train_num_sents = len(list(set(dataset_sel.train.sentence.values.tolist())))\n",
    "\n",
    "test_num_rows = dataset_sel.test.shape[0]\n",
    "test_num_sents = len(list(set(dataset_sel.test.sentence.values.tolist())))\n",
    "\n",
    "dataset = dataset_sel.train.append(dataset_sel.test)\n",
    "dataset['sent_id'] = dataset.groupby('sentence').ngroup()\n",
    "dataset_num_rows = dataset.shape[0]\n",
    "dataset_num_sents = len(list(set(dataset.sentence.values.tolist())))\n",
    "\n",
    "print('# Rows train : {}'.format(train_num_rows))\n",
    "print('# Rows test : {}'.format(test_num_rows))\n",
    "print('# Rows dataset : {}'.format(dataset_num_rows))\n",
    "\n",
    "print('# Sents train : {}'.format(train_num_sents))\n",
    "print('# Sents test : {}'.format(test_num_sents))\n",
    "print('# Sents dataset : {}'.format(dataset_num_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbl = dict.fromkeys(i for i in range(sys.maxunicode)\n",
    "                      if unicodedata.category(chr(i)).startswith('P'))\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(tbl)\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def all_tokens_with_index(context):\n",
    "    curr_pos = 0\n",
    "    targets = []\n",
    "    j = 0\n",
    "    w = 0\n",
    "    curr_split = ''\n",
    "    ctx_split = context.split()\n",
    "    whitespaces = re.findall('\\s+', context)\n",
    "    num_whitespaces = [len(token) for token in whitespaces]\n",
    "    num_whitespaces.append(1)\n",
    "    tokens = word_tokenize(context)\n",
    "    tokens = ['\"' if token not in context else token for token in tokens]\n",
    "    for index, token in enumerate(tokens, 1):\n",
    "        targets.append((token, index, curr_pos, (curr_pos + len(token))))\n",
    "        curr_pos += len(token)\n",
    "        curr_split += token\n",
    "        if ctx_split[j] == curr_split:\n",
    "            curr_pos += num_whitespaces[w]\n",
    "            j += 1\n",
    "            w += 1\n",
    "            curr_split = ''\n",
    "    return [val for val in targets if val[0] != '\"']\n",
    "\n",
    "def build_vocabulary(sentences, embedding_model, dimension):\n",
    "    all_words = [tpl[0] for sentence in sentences for tpl in sentence['seq']] + list(wordlist_lowercased)\n",
    "    print('# Words : {}'.format(len(all_words)))\n",
    "    counter = Counter(all_words)\n",
    "    vocab_size = len(counter) + 1\n",
    "    print('# Vocab : {}'.format(vocab_size))\n",
    "    print('# embeding model  : {}'.format(len(embedding_model)))   \n",
    "    word2index = {word : index for index, (word, count) in enumerate(counter.most_common(), 1)}\n",
    "    index2word = {index : word for word, index in word2index.items()}\n",
    "    # +1 required for pad token\n",
    "    embedding_matrix = np.zeros(((vocab_size), dimension))\n",
    "    missing_embed_words = []\n",
    "    i_ = 0\n",
    "    for word, index in word2index.items():\n",
    "        if word in embedding_model.key_to_index:\n",
    "            embedding = embedding_model[word]\n",
    "        else:\n",
    "             i_ +=1\n",
    "             missing_embed_words.append(word)\n",
    "             continue\n",
    "        embedding_matrix[index] = embedding\n",
    "    missing_embed_count = len(missing_embed_words)\n",
    "    print('# Words missing embedding : {}'.format(missing_embed_count))\n",
    "    print('Embedding shape : {}'.format(embedding_matrix.shape))\n",
    "    print(\"i: \", i_ )\n",
    "    return word2index, index2word, embedding_matrix\n",
    "\n",
    "def forward_transformation(dataframe, lowercase = True, filter_punc = True, filtering = \"a132\"):\n",
    "    grouped = dataframe.groupby('sentence').apply(lambda row : \n",
    "                        {'sent_id' : list(set(row['sent_id']))[0],\n",
    "                         'sentence' : list(set(row['sentence']))[0], \n",
    "                         'tags': [tag for tag in zip(row['target'], \n",
    "                            row['start'], row['end'], row['binary'], row['prob'])]})\n",
    "    sentences = []\n",
    "    for vals in grouped:\n",
    "        sent_id = vals['sent_id']\n",
    "        sentence = vals['sentence']\n",
    "        tags = vals['tags']\n",
    "        tags_without_labels = [(word, start, end) for word, start, end, binary, prob in tags]\n",
    "        all_tokens = all_tokens_with_index(sentence)\n",
    "        sent_repr = [(word, start, end, tags[tags_without_labels.index((word, start, end))][3],\n",
    "                     tags[tags_without_labels.index((word, start, end))][4])\n",
    "           if (word, start, end) in tags_without_labels \n",
    "          else (word, start, end, 0, 0.0) for word, index, start, end in all_tokens]\n",
    "        if lowercase:\n",
    "            sent_repr = [(word.lower(), start, end, binary, prob) \n",
    "                         for word, start, end, binary, prob in sent_repr]\n",
    "        if filter_punc:\n",
    "            sent_repr = list(filter(lambda vals : remove_punctuation(vals[0]), sent_repr))\n",
    "        if filtering:\n",
    "            sent_repr = list(filter(lambda vals : vals[0] != \"'s\", sent_repr))\n",
    "            sent_repr = list(filter(lambda vals : vals[0] != \"``\", sent_repr))\n",
    "        sentences.append({'sent_id' : sent_id, 'sentence' : sentence, 'seq' : sent_repr})\n",
    "    return sentences\n",
    "\n",
    "def split_sentence_seqs(sentences):\n",
    "    words, start_end, binary, prob = [], [], [] ,[]\n",
    "    for sent in sentences:\n",
    "        sequence = sent['seq']\n",
    "        curr_w, curr_se, curr_b, curr_p = map(list, zip(*[(vals[0], \n",
    "            (vals[1], vals[2]), vals[3], vals[4]) for vals in sequence]))\n",
    "        words.append(curr_w)\n",
    "        start_end.append(curr_se)\n",
    "        binary.append(curr_b)\n",
    "        prob.append(curr_p)\n",
    "    return words, start_end, binary, prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = forward_transformation(dataset)\n",
    "train_sentences = sentences[:train_num_sents]\n",
    "test_sentences = sentences[train_num_sents:]\n",
    "words, start_end, binary, prob = split_sentence_seqs(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Words : 96437\n",
      "# Vocab : 52455\n",
      "# embeding model  : 400000\n",
      "# Words missing embedding : 9834\n",
      "Embedding shape : (52455, 300)\n",
      "i:  9834\n"
     ]
    }
   ],
   "source": [
    "embedding_model = glove_models[0].model\n",
    "dimension = embedding_model.vector_size\n",
    "word2index, index2word, embedding = build_vocabulary(sentences, embedding_model, dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length sentence : 103\n"
     ]
    }
   ],
   "source": [
    "\n",
    "words_with_indices = [[word2index[word] for word in sent] for sent in words]\n",
    "sent_lens = [len(sentence['seq']) for sentence in sentences]\n",
    "sent_max_length = np.max(sent_lens)\n",
    "print('Max length sentence : {}'.format(sent_max_length))\n",
    "\n",
    "\n",
    "words_padded = pad_sequences(maxlen=sent_max_length, sequences=words_with_indices, padding=\"post\", value=0)\n",
    "binary_padded = pad_sequences(maxlen=sent_max_length, sequences=binary, padding=\"post\", value=0)\n",
    "prob_padded = pad_sequences(maxlen=sent_max_length, sequences=prob, padding=\"post\", value=0, dtype=\"float\")\n",
    "\n",
    "binary_padded_categorical = [to_categorical(clazz, num_classes=2) for clazz in binary_padded]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length sentence : 103\n"
     ]
    }
   ],
   "source": [
    "words_with_indices = [[word2index[word] for word in sent] for sent in words]\n",
    "sent_lens = [len(sentence['seq']) for sentence in sentences]\n",
    "sent_max_length = np.max(sent_lens)\n",
    "print('Max length sentence : {}'.format(sent_max_length))\n",
    "\n",
    "words_padded = pad_sequences(maxlen=sent_max_length, sequences=words_with_indices, padding=\"post\", value=0)\n",
    "binary_padded = pad_sequences(maxlen=sent_max_length, sequences=binary, padding=\"post\", value=0)\n",
    "prob_padded = pad_sequences(maxlen=sent_max_length, sequences=prob, padding=\"post\", value=0, dtype=\"float\")\n",
    "\n",
    "binary_padded_categorical = [to_categorical(clazz, num_classes=2) for clazz in binary_padded]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set length : 1988\n",
      "Test set length : 53\n"
     ]
    }
   ],
   "source": [
    "# (1) Training set\n",
    "train_words_padded = words_padded[:train_num_sents]\n",
    "train_binary_padded = binary_padded[:train_num_sents]\n",
    "train_binary_padded_categorical = binary_padded_categorical[:train_num_sents]\n",
    "train_prob_padded = prob_padded[:train_num_sents]\n",
    "train_start_end = start_end[:train_num_sents]\n",
    "\n",
    "# (2) Test set\n",
    "test_words_padded = words_padded[train_num_sents:]\n",
    "test_binary_padded = binary_padded[train_num_sents:]\n",
    "test_binary_padded_categorical = binary_padded_categorical[train_num_sents:]\n",
    "test_prob_padded = prob_padded[train_num_sents:]\n",
    "test_start_end = start_end[train_num_sents:]\n",
    "\n",
    "print('Training set length : {}'.format(len(train_words_padded)))\n",
    "print('Test set length : {}'.format(len(test_words_padded)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics(tensorflow.keras.callbacks.Callback):\n",
    "    def __init__(self, validation_data):\n",
    "        self.f1_scores = []\n",
    "        self.validation_data = validation_data\n",
    "        \n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        predict = np.asarray(self.model.predict(self.validation_data[0]))\n",
    "        targ = self.validation_data[1]\n",
    "        targ = np.array(targ)\n",
    "        shape = targ.shape\n",
    "        targ = targ.reshape((shape[0]*shape[1], shape[2]))\n",
    "        targ = np.argmax(targ, axis = 1)\n",
    "        predict = predict.reshape((shape[0]*shape[1]), shape[2])\n",
    "        predict = np.argmax(predict, axis = 1)\n",
    "        self.f1s=f1_score(targ, predict)\n",
    "        print(\"\\nF1 Score:\")\n",
    "        print(f1_score(targ, np.ones(shape[0]*shape[1])))\n",
    "        self.f1_scores.append(self.f1s)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1988, 103), (1988, 103, 2))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_words_padded.shape, np.array(train_binary_padded_categorical).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((52455, 300), 103)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.shape, sent_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 103)]             0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 103, 300)          15736500  \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 103, 300)          0         \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 103, 300)         541200    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " time_distributed (TimeDistr  (None, 103, 2)           602       \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16,278,302\n",
      "Trainable params: 16,278,302\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = embedding.shape[0]\n",
    "dimension = embedding.shape[1]\n",
    "\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "\n",
    "in_seq = Input(shape=(sent_max_length,))\n",
    "embed = Embedding(input_dim=vocab_size, output_dim=dimension, \\\n",
    "                  weights=[embedding], input_length=sent_max_length)(in_seq)\n",
    "drop = Dropout(0.1)(embed)\n",
    "lstm = Bidirectional(LSTM(units=150, return_sequences=True, recurrent_dropout=0.1))(drop)\n",
    "out = TimeDistributed(Dense(2, activation=\"softmax\"))(lstm) \n",
    "\n",
    "model = Model(in_seq, out)\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "model.summary()\n",
    "\n",
    "metrics = Metrics((test_words_padded, np.array(test_binary_padded_categorical)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "199/199 [==============================] - ETA: 0s - loss: 0.0889 - accuracy: 0.9688\n",
      "F1 Score:\n",
      "0.05660377358490566\n",
      "199/199 [==============================] - 84s 403ms/step - loss: 0.0889 - accuracy: 0.9688 - val_loss: 0.0459 - val_accuracy: 0.9795\n",
      "Epoch 2/3\n",
      "199/199 [==============================] - ETA: 0s - loss: 0.0404 - accuracy: 0.9828\n",
      "F1 Score:\n",
      "0.05660377358490566\n",
      "199/199 [==============================] - 78s 390ms/step - loss: 0.0404 - accuracy: 0.9828 - val_loss: 0.0439 - val_accuracy: 0.9819\n",
      "Epoch 3/3\n",
      "199/199 [==============================] - ETA: 0s - loss: 0.0301 - accuracy: 0.9874\n",
      "F1 Score:\n",
      "0.05660377358490566\n",
      "199/199 [==============================] - 78s 391ms/step - loss: 0.0301 - accuracy: 0.9874 - val_loss: 0.0446 - val_accuracy: 0.9809\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_words_padded, np.array(train_binary_padded_categorical), batch_size=10, \n",
    "                    epochs=3, validation_data = (test_words_padded, np.array(test_binary_padded_categorical)), \n",
    "                    verbose=1, callbacks=[metrics])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAAAyNElEQVR4nO3deXxV5bX/8c/KPJCJMMmMgjKPARyrVmtxxAEFKxQUpbb12t72tqXDbb3+2nvb3l5rW22VijiPWFvaaq1WrbVVICAggygyDzKEkDAkkJD1+2Nvwkk4QAI5ORm+79crr+yzn2efs7I5nHWe/ey9trk7IiIitSXEOwAREWmalCBERCQqJQgREYlKCUJERKJSghARkaiUIEREJColCJGTZGaPmNkP69h3rZldfLLPI9IYlCBERCQqJQgREYlKCUJahfDQzjfMbImZ7TWzmWbW0cxeNrPdZvaameVF9L/KzJaZ2S4ze9PM+kW0DTOzheF2zwJptV7rCjNbFG77LzMbfIIx32Zmq8xsp5nNMbPO4Xozs5+b2TYzKzWz981sYNh2mZktD2PbZGb/cUI7TAQlCGldrgM+A5wOXAm8DHwHaE/wf+FOADM7HXga+GrY9hLwRzNLMbMU4PfA40Bb4PnweQm3HQY8DHwByAceBOaYWWp9AjWzTwP/A9wAnAKsA54Jmy8BPhX+HTlhn6KwbSbwBXfPAgYCr9fndUUiKUFIa/Ird9/q7puAfwBz3f09dy8HXgSGhf3GA39291fdvQL4GZAOnA2cCSQD97p7hbvPBuZHvMY04EF3n+vuB939UWB/uF193AQ87O4L3X0/8G3gLDPrCVQAWUBfwNx9hbtvCberAPqbWba7F7v7wnq+rkg1JQhpTbZGLJdFedwmXO5M8I0dAHevAjYAXcK2TV6zyuW6iOUewNfDw0u7zGwX0C3crj5qx7CHYJTQxd1fB+4D7ge2mdkMM8sOu14HXAasM7O/m9lZ9XxdkWpKECJH2kzwQQ8Ex/wJPuQ3AVuALuG6Q7pHLG8AfuTuuRE/Ge7+9EnGkElwyGoTgLv/0t1HAP0JDjV9I1w/393HAh0IDoU9V8/XFammBCFypOeAy83sIjNLBr5OcJjoX8A7QCVwp5klm9m1wKiIbX8L3G5mo8PJ5Ewzu9zMsuoZw9PAzWY2NJy/+G+CQ2JrzWxk+PzJwF6gHKgK50huMrOc8NBYKVB1EvtBWjklCJFa3H0lMBH4FbCDYEL7Snc/4O4HgGuBKcBOgvmK30VsWwjcRnAIqBhYFfatbwyvAf8JvEAwajkNmBA2ZxMkomKCw1BFwP+GbZOAtWZWCtxOMJchckJMNwwSEZFoNIIQEZGolCBERCQqJQgREYlKCUJERKJKincADaVdu3bes2fPeIchItKsLFiwYIe7t4/W1mISRM+ePSksLIx3GCIizYqZrTtamw4xiYhIVDFNEGY2xsxWhiWLp0dpTzWzZ8P2uWEhMsIrQmeFZYwXm9kFsYxTRESOFLMEYWaJBMXELiWoF3OjmfWv1W0qUOzuvYGfAz8J198G4O6DCMoz/5+ZabQjItKIYjkHMQpY5e6rAczsGWAssDyiz1jgrnB5NnBfWAStP2Ede3ffFlbELADm1SeAiooKNm7cSHl5+Un8GRIpLS2Nrl27kpycHO9QRCTGYpkguhBUtjxkIzD6aH3cvdLMSggqVi4GrjKzpwmqaI4If9crQWzcuJGsrCx69uxJzeKbciLcnaKiIjZu3EivXr3iHY6IxFhTPWzzMEFCKQTuJaiiebB2JzObZmaFZla4ffv2I56kvLyc/Px8JYcGYmbk5+drRCbSSsQyQWwi+NZ/SNdwXdQ+ZpZEcPvEInevdPd/d/ehYW37XODD2i/g7jPcvcDdC9q3j3oar5JDA9P+FGk9Ypkg5gN9zKxXeB/fCcCcWn3mAJPD5XHA6+7uZpYR3iAFM/sMUOnuy4mBg1XO5l1lVFapbL6ISKSYJQh3rwTuAF4BVgDPufsyM7vbzK4Ku80E8s1sFfA14NCpsB2AhWa2AvgWQY37mCivOEjR3gOs27GPqqqGL32+a9cufv3rX9d7u8suu4xdu3Y1eDwiInXVYu4HUVBQ4LWvpF6xYgX9+vU77rYl+w6wfuc+2qQl0yM/g4QGPIyydu1arrjiCpYuXVpjfWVlJUlJzfNC9rruVxFp+sxsgbsXRGtrqpPUjSonI4UueRnsLq9gw859NGTSnD59Oh9//DFDhw5l5MiRnHfeeVx11VX07x9cEnL11VczYsQIBgwYwIwZM6q369mzJzt27GDt2rX069eP2267jQEDBnDJJZdQVlbWYPGJiBxN8/wKewL+64/LWL659Jh9Kg5WcaCyiqTEBFKTjp87+3fO5gdXDjhmnx//+McsXbqURYsW8eabb3L55ZezdOnS6tNEH374Ydq2bUtZWRkjR47kuuuuIz8/v8ZzfPTRRzz99NP89re/5YYbbuCFF15g4sSJx41PRORktJoEURfJiQk4UFFZhRmkJDb8AGvUqFE1riH45S9/yYsvvgjAhg0b+Oijj45IEL169WLo0KEAjBgxgrVr1zZ4XCIitbWaBHG8b/qHuDubS8op2rOfTjlpdMhKa9A4MjMzq5fffPNNXnvtNd555x0yMjK44IILol5jkJqaWr2cmJioQ0wi0ihaTYKoKzOjc04aB6ucT0rKSTQjv03q8Tc8iqysLHbv3h21raSkhLy8PDIyMvjggw949913T/h1REQamhJEFGZG17x0qqqcTbvKSEwwcjNSTui58vPzOeeccxg4cCDp6el07Nixum3MmDE88MAD9OvXjzPOOIMzzzyzof4EEZGTptNcj6GqyllTtJd9+w/SIz+D7HQVqAOd5irSkug01xOUkGD0zM8gLTmB9Tv3sWd/ZbxDEhFpNEoQx5GYkECvdpkkJyawbsdeyg4oSYhI66AEUQdJiUGSSEww1uzYR3nFEYVlRURaHCWIOkpJCpIEwJodezlQqeJ+ItKyKUHUQ2pyIr3aZVLlzpode6k4qCQhIi2XEkQ9pack0jM/k4qDVazdsVdlwkWkxVKCOAGZqUn0yM+gvLKqwcuEt2nTBoDNmzczbty4qH0uuOACap/SW9u9997Lvn37qh+rfLiI1JcSxAnKSkumW146+w5Usm7nPqoa+HqSzp07M3v27BPevnaCeOmll8jNzW2AyESktVCCOAm5GSl0yUtnd3kFG49SJnz69Oncf//91Y/vuusufvjDH3LRRRcxfPhwBg0axB/+8Icjtlu7di0DBw4EoKysjAkTJtCvXz+uueaaGrWYvvjFL1JQUMCAAQP4wQ9+AAQFADdv3syFF17IhRdeCBwuHw5wzz33MHDgQAYOHMi9995b/XoqKy4ikVpPqY2Xp8Mn7zfsc3YaRNtLf8zBKmdLSTkJu8rokpte477N48eP56tf/Spf/vKXAXjuued45ZVXuPPOO8nOzmbHjh2ceeaZXHXVVUe93/NvfvMbMjIyWLFiBUuWLGH48OHVbT/60Y9o27YtBw8e5KKLLmLJkiXceeed3HPPPbzxxhu0a9euxnMtWLCAWbNmMXfuXNyd0aNHc/7555OXl6ey4iJSg0YQDaB9VhodslLZufcAn5TWrMY6bNgwtm3bxubNm1m8eDF5eXl06tSJ73znOwwePJiLL76YTZs2sXXr1qM+/1tvvVX9QT148GAGDx5c3fbcc88xfPhwhg0bxrJly1i+/Ni37n777be55ppryMzMpE2bNlx77bX84x//AFRWXERqaj0jiEt/HNOn75gdVIDdvns/iQlWo0z49ddfz+zZs/nkk08YP348Tz75JNu3b2fBggUkJyfTs2fPqGW+j2fNmjX87Gc/Y/78+eTl5TFlypQTep5DVFZcRCJpBNFAzIzOuenkpqfwSXg/iUPGjx/PM888w+zZs7n++uspKSmhQ4cOJCcn88Ybb7Bu3bpjPvenPvUpnnrqKQCWLl3KkiVLACgtLSUzM5OcnBy2bt3Kyy+/XL3N0cqMn3feefz+979n37597N27lxdffJHzzjuvIXaBiLQwrWcE0QjMjK5t06kqqlkmfMCAAezevZsuXbpwyimncNNNN3HllVcyaNAgCgoK6Nu37zGf94tf/CI333wz/fr1o1+/fowYMQKAIUOGMGzYMPr27Uu3bt0455xzqreZNm0aY8aMoXPnzrzxxhvV64cPH86UKVMYNWoUALfeeivDhg3T4SQROYLKfcdAjTLh7TLITmtZZcJV7luk5VC570ZWo0x40T72qky4iDRDShAxElkmfK3KhItIM9TiE0Q8D6G1xDLhLeWQpIgcX0wThJmNMbOVZrbKzKZHaU81s2fD9rlm1jNcn2xmj5rZ+2a2wsy+fSKvn5aWRlFRUVw/1FpSmXB3p6ioiLS0tON3FpFmL2ZnMZlZInA/8BlgIzDfzOa4e+SVXFOBYnfvbWYTgJ8A44HrgVR3H2RmGcByM3va3dfWJ4auXbuyceNGtm/f3hB/0kmpPFjF9t372breaNcmlcSE6FdNN3VpaWl07do13mGISCOI5Wmuo4BV7r4awMyeAcYCkQliLHBXuDwbuM+CehMOZJpZEpAOHABK6xtAcnIyvXr1OuE/oKEVrt3JxJlzOa19G56edmaLO7tJRFqWWB5i6gJsiHi8MVwXtY+7VwIlQD5BstgLbAHWAz9z9521X8DMpplZoZkVNoVRwvEU9GzLAxNH8OHW3dz6SCFlB5r/nISItFxNdZJ6FHAQ6Az0Ar5uZqfW7uTuM9y9wN0L2rdv39gxnpALzujAPTcMZf66nXzpyQXNek5CRFq2WCaITUC3iMddw3VR+4SHk3KAIuBzwF/cvcLdtwH/BKJeyNEcXTmkM/99zSDeWLmdrz+/mIMNeMMhEZGGEssEMR/oY2a9zCwFmADMqdVnDjA5XB4HvO7BKUfrgU8DmFkmcCbwQQxjbXQ3jurO9Ev78sfFm/nPPyzV6aMi0uTEbJLa3SvN7A7gFSAReNjdl5nZ3UChu88BZgKPm9kqYCdBEoHg7KdZZrYMMGCWuy+JVazxcvv5p1FSVsFv3vyY3PRkvjnm2DWZREQaU0yL9bn7S8BLtdZ9P2K5nOCU1trb7Ym2viX65mfPoLSsgl+/+TE56cl84fzT4h2SiAigaq5xZ2bcPXYgpeWV/M/LH5CdnsyNo7rHOywRESWIpiAxwfi/64ewu7yC77z4PllpSVwxuHO8wxKRVq6pnuba6qQkJfCbm0Ywskdb/v3ZRby5clu8QxKRVk4JoglJT0nkoSkFnN4xi9ufWMD8tUdcGygi0miUIJqY7LRkHr1lFJ1z0rnlkfks21wS75BEpJVSgmiC2rVJ5fFbR5OVmsTkh+exevueeIckIq2QEkQT1SU3ncdvHY07TJo5j827yuIdkoi0MkoQTdhp7dvw6C2jKC2rYOLMuRTt2R/vkESkFVGCaOIGdslh5pSRbCouY/KseZSWV8Q7JBFpJZQgmoFRvYIy4R9s2c2tjxa2iFuXikjTpwTRTFzYtwM/Hz+U+Wt38qUnF1JxUGXCRSS2lCCakSuHdOaHVw/k9Q+28R/PL6ZKZcJFJIZUaqOZuWl0D0rKKvjpX1aSlZbE/xs7kOAurSIiDUsJohn60gW9KSmr4MG/ryYnPZlvfFZlwkWk4SlBNFPTx/SltKyS+98IyoRP+5TKhItIw1KCaKbMjB9ePZDd5RX890sfkJ2WzASVCReRBqQE0YwlJhj33DCUPfsr+c6L75Odnsxlg06Jd1gi0kLoLKZm7lCZ8OHd8/jKM+/x9w+3xzskEWkhlCBagPSURGZOGUmfDlnc/vgCFqxTmXAROXlKEC1ETnpQJrxTThpTZs1n+ebSeIckIs2cEkQL0j4rlSduHU2b1CQ+//Bc1uzYG++QRKQZU4JoYbrkpvP41NFUOUx8aC5bSlQmXEROjBJEC9S7Qxseu2UUJWUVTJo5j517D8Q7JBFphpQgWqiBXXKYObmADTv3MfnheexWmXARqScliBZs9Kn5/GbicFZsKVWZcBGpt5gmCDMbY2YrzWyVmU2P0p5qZs+G7XPNrGe4/iYzWxTxU2VmQ2MZa0v16b4d+b8bhjBv7U6+rDLhIlIPMUsQZpYI3A9cCvQHbjSz/rW6TQWK3b038HPgJwDu/qS7D3X3ocAkYI27L4pVrC3d2KFd+H9jB/I3lQkXkXqI5QhiFLDK3Ve7+wHgGWBsrT5jgUfD5dnARXZk7eobw23lJEw8swff+OwZ/GHRZu764zLclSRE5NhiWYupC7Ah4vFGYPTR+rh7pZmVAPnAjog+4zkysQBgZtOAaQDdu6tQ3fF86YLTKC2r4MG3gjLhX7/kjHiHJCJNWJMu1mdmo4F97r40Wru7zwBmABQUFOgr8XGYGdMv7UtJWQW/en0VOenJ3HreqfEOS0SaqFgmiE1At4jHXcN10fpsNLMkIAcoimifADwdwxhbHTPjR9cMYnd5JT/88wqy05K5YWS3428oIq1OLBPEfKCPmfUiSAQTgM/V6jMHmAy8A4wDXvfw4LiZJQA3AOfFMMZWKTHB+Pn4oezeX8n03y2hTVqSyoSLyBFiNknt7pXAHcArwArgOXdfZmZ3m9lVYbeZQL6ZrQK+BkSeCvspYIO7r45VjK1ZSlICD0wcXl0m/C2VCReRWqylnM1SUFDghYWF8Q6j2Skpq2DCjHdZu2MvT9w6mhE98uIdkog0IjNb4O4F0dp0JXUrl5OezGO3jKJjdio3z5rHii0qEy4iASUIqS4TnpGSxKSZ81irMuEighKEhLrmZfDEraOocuemh+bySUl5vEMSkThTgpBqvTtk8ejNQZnwiTPnqky4SCunBCE1DOqaw0NhmfAps+axZ39lvEMSkThRgpAjnHlqPr++aTjLNpdym8qEi7RaShAS1UX9OvJ/1w/h3TVF3PHUeyoTLtIKKUHIUV09rAt3XzWA11Zs5Zuzl6hMuEgr06SL9Un8TTqrJyVlFfzsrx+SnZbEXVcN4MiK7CLSEilByHF9+cLelJRV8Nt/rCEnPZmvqUy4SKugBCHHZWZ857J+lJZV8svXV5GtMuEirYIShNSJmfHf1w6itLwiKBOenswNBSoTLtKSaZJa6iwxwbh3wlDO69OO6S8s4S9Lt8Q7JBGJISUIqZfUpEQenDSCod1yufPpRbz90Y7jbyQizZIShNRbRkoSs6aM4tT2mUx7vJCF64vjHZKIxIAShJyQnIxkHps6ig5ZqUx5eB4ffKIy4SItjRKEnLAOWWk8PnU06SmJTJo5j3VFKhMu0pIoQchJ6dY2gyemjqbyYBUTZ85la6nKhIu0FEoQctL6dMzikZtHsXPPASY+NJdilQkXaRGUIKRBDOmWy0OTR7JOZcJFWgwlCGkwZ52Wz68/N5ylKhMu0iIoQUiDurh/R352/WDeWV3Evz39HpUqEy7SbClBSIO7ZlhX/uuqAby6fCvffEFlwkWaqzolCDP7ipllW2CmmS00s0tiHZw0X5PP7snXPnM6v1u4ibv/tBx3JQmR5qauI4hb3L0UuATIAyYBPz7eRmY2xsxWmtkqM5sepT3VzJ4N2+eaWc+ItsFm9o6ZLTOz980srY6xShPxb5/uzdRze/HIv9Zy72sfxTscEamnulZzPXSHmMuAx919mR3nrjFmlgjcD3wG2AjMN7M57r48ottUoNjde5vZBOAnwHgzSwKeACa5+2Izywcq6v5nSVNgZnzv8n6UllXwi799RE56Mrec2yveYYlIHdV1BLHAzP5KkCBeMbMs4Hizj6OAVe6+2t0PAM8AY2v1GQs8Gi7PBi4KE88lwBJ3Xwzg7kXurlNimiEz43+uHcSYAZ24+0/Leb5wQ7xDEpE6qmuCmApMB0a6+z4gGbj5ONt0ASI/DTaG66L2cfdKoATIB04H3MxeCec7vlnHOKUJSkpM4Bc3DuXc3u341gtLeGXZJ/EOSUTqoK4J4ixgpbvvMrOJwPcIPsxjJQk4F7gp/H2NmV1Uu5OZTTOzQjMr3L59ewzDkZN1qEz4kG65/NtT7/HPVSoTLtLU1TVB/AbYZ2ZDgK8DHwOPHWebTUDkLce6huui9gnnHXKAIoLRxlvuviMcsbwEDK/9Au4+w90L3L2gffv2dfxTJF4yU5OYNWUkvdplcttjhbynMuEiTVpdE0SlB+cpjgXuc/f7gazjbDMf6GNmvcwsBZgAzKnVZw4wOVweB7wevs4rwCAzywgTx/nAcqTZy81I4fGpo2iflcqUWfNZ+cnueIckIkdR1wSx28y+TXB665/NLIFgHuKowjmFOwg+7FcAz4VnP91tZleF3WYC+Wa2CvgawTwH7l4M3EOQZBYBC939z/X6y6TJ6pCdxhNTR5OWnMCkmXNZX7Qv3iGJSBRWlwuYzKwT8Dlgvrv/w8y6Axe4+/EOMzWagoICLywsjHcYUg8fbt3NDQ++Q1ZaErNvP5uO2brURaSxmdkCdy+I1lanEYS7fwI8CeSY2RVAeVNKDtI8nR5RJnzSzLns2qcy4SJNSV1LbdwAzAOuB24A5prZuFgGJq3D0G65/PbzBawt2seUWfPZqzLhIk1GXecgvktwDcRkd/88wUVw/xm7sKQ1Obt3O+67cRjvbyph2uMqEy7SVNQ1QSS4+7aIx0X12FbkuC4Z0ImfXjeYf64q4k6VCRdpEur6If+X8KrmKWY2BfgzwbUJIg3muhFduevK/vx1+Va+9cL7KhMuEmd1Ktbn7t8ws+uAc8JVM9z9xdiFJa3VlHN6UVJWyc9f+5Ds9CS+f0V/jlMXUkRipK7VXHH3F4AXYhiLCAB3XtSbXWUHmPXPteSmp/CVi/vEOySRVumYCcLMdgPRxvkGuLtnxyQqadXMjP+8vD+7yw+PJG4+R2XCRRrbMROEux+vnIZITCQkGD++dhC7yyv4rz8uJzstmetGdI13WCKtis5EkiYrKTGBX0wYxjm98/nmC0v4q8qEizQqJQhp0tKSE5kxqYBBXXK446n3+JfKhIs0GiUIafIyU5N45OagTPitjxWyaMOueIck0iooQUizkJuRwmNTR9GuTSpTZs3jw60qEy4Sa0oQ0mx0DMuEpyQGZcI37FSZcJFYUoKQZqV7fgaPTx1NeUUVNz00l22l5fEOSaTFUoKQZueMTlk8cvNIduzZz6SZ81QmXCRGlCCkWRrWPY/ffr6ANTv2qky4SIwoQUizdU7vdvzyxmEs2biLLzy+gP2VKhMu0pCUIKRZGzOwEz8dN4S3V+3gK08vUplwkQakBCHN3rgRXfn+Ff35y7JP+PbvVCZcpKHUuZqrSFN2y7m9KCmr4Bd/+4js9GS+d3k/lQkXOUlKENJifPXiPpSUVTDz7TXkpCdz50UqEy5yMpQgpMUwM75/RX9Kyyu459UPyU5LYorKhIucMCUIaVESEoyfXjeY3eWV3PXH5eRkJHPNMJUJFzkRmqSWFicpMYFf3TiMs0/L5z+eX8Kry7fGOySRZimmCcLMxpjZSjNbZWbTo7SnmtmzYftcM+sZru9pZmVmtij8eSCWcUrLk5acyIzPFzCwSw5ffmoh//pYZcJF6itmCcLMEoH7gUuB/sCNZta/VrepQLG79wZ+Dvwkou1jdx8a/tweqzil5WqTmsQjU0bSMz+D2x4tZLHKhIvUSyxHEKOAVe6+2t0PAM8AY2v1GQs8Gi7PBi4ynZsoDSgvM4XHp44mLzOFybPm8ZHKhIvUWSwTRBdgQ8TjjeG6qH3cvRIoAfLDtl5m9p6Z/d3Mzov2AmY2zcwKzaxw+/btDRu9tBgds9N48tbRJCcmMFFlwkXqrKlOUm8Burv7MOBrwFNmll27k7vPcPcCdy9o3759owcpzUeP/EwenzqK8ooqJs6cy7bdKhMucjyxTBCbgG4Rj7uG66L2MbMkIAcocvf97l4E4O4LgI+B02MYq7QCfTtlM+vmkWzfvZ/Pz5xHyb6KeIck0qTFMkHMB/qYWS8zSwEmAHNq9ZkDTA6XxwGvu7ubWftwkhszOxXoA6yOYazSSgzvnseMSQWs3r6Xmx+Zx74DKhMucjQxSxDhnMIdwCvACuA5d19mZneb2VVht5lAvpmtIjiUdOhU2E8BS8xsEcHk9e3uvjNWsUrrcm6fdvzyxqEs2qAy4SLHYu4to/JlQUGBFxYWxjsMaUaeK9zAN2cv4dKBnfjVjcNISmyqU3IisWNmC9y9IFqb/kdIq3VDQTe+d3k/Xl76Cd958X1aypclkYaiWkzSqt163qmUllXwy9dXkZ2WzHdVJlykmhKEtHr//pnTKS2v5KG315Cbkcwdn1aZcBFQghA5XCa8rIKf/fVDstOT+fxZPeMdlkjcKUGIEJQJ/8m4wZSWV/L9PywjOy2Zq4fVvvBfpHXRJLVIKDkxgfs+N4wzT23L159fzGsqEy6tnBKESIS05EQemjySAZ2z+dJTC3nn46J4hyQSN7oOongtvHYXJGdCSgYkZ0BKm4jlzMO/q5fDPslhnwTl2ZZm594DjH/wHbaUlPPUbaMZ3DU33iGJxMSxroPQHMT+PbB1GRzYBwf2QMU+OHigfs+RlB4mjcx6Jppay0ckn3TQKZdx0TYsEz7ugX8x+eF5PH/7WfTukBXvsEQalUYQ0RyshIq9cGBvkDgqIn/XWndgb0R7+PjA3sPL1evCbarqU/vHIhJJxrGTz7ESTfW2Ef2SUpV86mDtjr2Me+AdkhKM528/i25tM+IdkkiDOtYIQgmisVUeODxSOWai2VMz6RyRfPYd+TxeVfc4LCEiaUQknyMSTbTkU+twW+3nSUqJ3f6LgxVbShn/4Du0zUzhudvPokNWWrxDEmkwShCtgTtU7q+VaE5mxLO3ZvKpj4SkKIfbTiDRRBsFJcbnqOiCdcVMfGguPfIzeHbaWeRkJMclDmkB3IMjCVWVUHWw1u/KKG0Rjz3KuqpKaNMJuo44oXCUIOTkVFVBZVkdE80xRkG1k86BfcHz1kdiygnM9UQ73FZ7zijzuCcb/OOj7dzyyHwGd83l8amjyEjRFF69VVUd+4PQD9b9w/KI9UfrU4cP2GM+rsc2UeOv9bg+I/26GnAtXD/rhDbVJLWcnISEwx+4NPCd+6oO1koated9jpVoIhLOnm1HPs+JnmxwlJHPeSkZvNrXeGllCX++/1muGXU6SWltjj8KSg7nLbz2h+PxPgjr+WHp9fywrM8H36EPtjp/cB/6qbUNTeALqSUEo9zqn8QjH1visduTUmu1R+lzrMdW320Sj90nPS8mu0oJQuIrIRFSs4KfhlZ9skHtEU89531KN1cnn54H9nJ78l4SSirh1YYPOeZO9kMqMTk4u64u/e04H2rH/PA73jZJdfigj/LclqjT0utBCUJarsQkSMyBtJwGfdoE4OG/r+TelxcxYUg+3764G1Y9ejlKosHq+E2z9ofZ8T746vEt1hJ05prUixKEyAm45fwz2FkO972xCnKq+PalQ1UmXFocJQiRE/T1S06ntLyCGW+tJic9mS9f2DveIYk0KCUIkRNkZtx15QBKyyr431dWkp2ezKQze8Q7LJEGowQhchISEoz/vX4Ie/ZX8v0/LCU7LYmxQ1UmXFoGTeeLnKSgTPhwRvVsy9efW8zrH6hMuLQMShAiDSAoE15Av1Oy+eITC3l3tcqES/OnBCHSQLLSknn0llF0zUvn1kcLeX9jSbxDEjkpShAiDahtZgpP3DqanPRkJs+ax6pte+IdksgJU4IQaWCn5KTzxK2jSTBj0sy5bCzeF++QRE5ITBOEmY0xs5VmtsrMpkdpTzWzZ8P2uWbWs1Z7dzPbY2b/Ecs4RRpar3aZPHbLKPbur2TSzHkUrt3J/sqD8Q5LpF5idpqrmSUC9wOfATYC881sjrsvj+g2FSh2995mNgH4CTA+ov0e4OVYxSgSS/07ZzPr5pFMmjmPcQ+8Q0piAoO65jCiRx7Du+cxokce7bNS4x2myFHF8jqIUcAqd18NYGbPAGOByAQxFrgrXJ4N3Gdm5u5uZlcDa4B63oxApOkY0aMtb3/r08xfu5OF64opXFfMI/9cy4y3VgPQIz+DEd3zGN4jSBind8wiMUElO6RpiGWC6AJsiHi8ERh9tD7uXmlmJUC+mZUD3yIYfRz18JKZTQOmAXTv3r3hIhdpQG0zU/jsgE58dkAnAPZXHmTpplIWritmwbpi3vpoB797bxMAbVKTGNY9t3qEMbR7LtlpujmRxEdTvZL6LuDn7r7nWAXQ3H0GMAOCGwY1TmgiJyc1KZER4YjhNsDd2VhcxoIwYRSuK+ZXr39ElQfFV8/omFXdf0SPPLq3zVBhQGkUsUwQm4BuEY+7huui9dloZklADlBEMNIYZ2Y/BXKBKjMrd/f7YhivSFyYGd3aZtCtbQZXDwvKdOwur2DxhpIgaawvZs6izTw5dz0A7dqkVI8wRvTIY2CXHNKSE+P5J0gLFcsEMR/oY2a9CBLBBOBztfrMASYD7wDjgNc9uAfqeYc6mNldwB4lB2lNstKSObdPO87t0w6Ag1XOqm17qkcZC9bt5K/Lg5IeKYkJDOySXZ0whnfPo0N2WjzDlxYiZgkinFO4A3gFSAQedvdlZnY3UOjuc4CZwONmtgrYSZBERKSWxATjjE5ZnNEpi8+NDubbduzZH8xjrC9m4bpiHn1nHb/9xxoAurVNZ0Q4yhjeI4++nbI1+S31ZsEX9uavoKDACwsL4x2GSNwcqKxi2eaSGnMZ23fvByAzJZGh3XMZ0aNtMPndLZecdE1+C5jZAncviNqmBCHSMh2a/F64vrg6aazYUlo9+X16h6zq02tH9MijZ74mv1sjJQgRAWDv/koWb9hVPfm9cF0xpeWVAORnptRIGIM0+d0qHCtBNNXTXEUkBjJTkzi7dzvO7h1MfldVOau2H578XriumFfDye/kRGNA55wap9h21OR3q6IRhIjUULRnP++t38WC9cUsWFvM4o272F9ZBUCX3HRG9MijoGdwtlTfTlkkJarmZ3OmEYSI1Fl+m1Qu7t+Ri/t3BILJ7+VbSqtHGHPXFDFn8WYAMlISGdott/psqeHd8sjJ0OR3S6ERhIjUi7uzuaS8OmEsWFfM8i2lHKwKPkv6dGhTPcIY0SOPXu0yNfndhGmSWkRiau/+ShZv3FWdMBau30VJWQUAeRnJ1SOMEd3zGNw1l/QUTX43FTrEJCIxlZmaxNmntePs0w5Pfq/eEXnldzGvrdgGQFKCMaBzNsN75FEQXpfRKUeT302RRhAi0ih27j3AexHXZCzeuIvyisOT38EII7iYr98pmvxuLBpBiEjctc1M4aJ+HbmoXzD5XXGwihXh5PeCdcUUrt3JH8PJ7/TkRIZ0y6keYQzrnktuRko8w2+VNIIQkSZj867DZc8Xri9m2ebDk9+9O7SpUV/qtPaa/G4ImqQWkWZp34FKlmwsOXzG1Ppidu0LJr9zM5JrlD0fosnvE6JDTCLSLGWkJHHmqfmceWo+EJxi+/H2vdVnSy1YX8zrHxye/O7fObtG0uicmx7P8Js9jSBEpFnbte9AcOV3mDQWbdhFWcVBAE7JSatRKqTfKdkka/K7Bo0gRKTFys1I4cK+HbiwbwcgmPz+YMtuFqzbyYL1wbUZf1qyBYC05ASGdM2tcXOlvExNfh+NRhAi0uJtKSlj4brDVWyXbSqhMpz8PrV9JiO6B/WlRvTI49R2bUhoRTdX0iS1iEiEsgMHWbJxV3XJ8wXriikOJ79z0pMZ3v1wfamh3XLJSGm5B1t0iElEJEJ6SiKjT81ndMTk95ode6tPr12wrpg3Vm4Hgtu99jsli4Iebavvl9E5J61VnGKrEYSISBQl+ypYuOHwCGPRhl3sOxBMfnfKTjtcX6pHHv1PySYlqXlOfmsEISJSTzkZyVx4RgcuPCOY/K48WMUHn+yucQvXP78fTH6nJgWT30F9qSBxtG0Bk98aQYiInKCtpeU1ChIu21xCxcFw8rtdZo1buPZu3zQnvzVJLSLSCMorDvL+ppIat3At2nsAgOy0JIZ1D0YYI3rkMaRbLpmp8T+Io0NMIiKNIC05kZE92zKyZ1sgmPxeW7SvRsK457UPcYcEg36nZNe4JqNrXnqTmvzWCEJEpBGVlFWwaMOu6oTx3vpi9oaT3x2yUmtc+T2gc07MJ7/jNoIwszHAL4BE4CF3/3Gt9lTgMWAEUASMd/e1ZjYKmHGoG3CXu78Yy1hFRBpDTnoy55/envNPbw/AwSpn5Sfhld/hhXwvL/0EgJSkBIZ0zam+G9/wHnm0a5PaaLHGbARhZonAh8BngI3AfOBGd18e0edLwGB3v93MJgDXuPt4M8sADrh7pZmdAiwGOrt75dFeTyMIEWkptpWW1zhbaummUg4cDG6u1DM/o8bd+Pp0OLnJ73iNIEYBq9x9dRjEM8BYYHlEn7HAXeHybOA+MzN33xfRJw1oGcfBRETqoEN2GmMGnsKYgacAweT30ojJ77c+3M7vFm4CICstifEF3fjeFf0bPI5YJoguwIaIxxuB0UfrE44WSoB8YIeZjQYeBnoAk6KNHsxsGjANoHv37g3+B4iINAVpyYkU9GxLQcTk9/qdhye/Y1XWvMmexeTuc4EBZtYPeNTMXnb38lp9ZhDOVRQUFGiUISKtgpnRIz+THvmZXDu8a8xeJ5bT45uAbhGPu4brovYxsyQgh2Cyupq7rwD2AANjFqmIiBwhlgliPtDHzHqZWQowAZhTq88cYHK4PA543d093CYJwMx6AH2BtTGMVUREaonZIaZwTuEO4BWC01wfdvdlZnY3UOjuc4CZwONmtgrYSZBEAM4FpptZBVAFfMndd8QqVhEROZIulBMRacWOdZpr86xPKyIiMacEISIiUSlBiIhIVEoQIiISVYuZpDaz7cC6k3iKdkBTPFNKcdWP4qofxVU/LTGuHu7ePlpDi0kQJ8vMCo82kx9Piqt+FFf9KK76aW1x6RCTiIhEpQQhIiJRKUEcNuP4XeJCcdWP4qofxVU/rSouzUGIiEhUGkGIiEhUShAiIhJVi08QZjbGzFaa2Sozmx6lPdXMng3b55pZz4i2b4frV5rZZxs5rq+Z2XIzW2JmfwvLnh9qO2hmi8Kf2iXUYx3XFDPbHvH6t0a0TTazj8KfybW3jXFcP4+I6UMz2xXRFsv99bCZbTOzpUdpNzP7ZRj3EjMbHtEWy/11vLhuCuN538z+ZWZDItrWhusXmVmDVsCsQ1wXmFlJxL/X9yPajvkeiHFc34iIaWn4nmobtsVyf3UzszfCz4JlZvaVKH1i9x5z9xb7Q1Bm/GPgVCAFWAz0r9XnS8AD4fIE4NlwuX/YPxXoFT5PYiPGdSGQES5/8VBc4eM9cdxfU4D7omzbFlgd/s4Ll/MaK65a/f+NoLx8TPdX+NyfAoYDS4/SfhnwMmDAmcDcWO+vOsZ19qHXAy49FFf4eC3QLk776wLgTyf7HmjouGr1vZLg3jWNsb9OAYaHy1nAh1H+T8bsPdbSRxCjgFXuvtrdDwDPAGNr9RkLPBouzwYuMjML1z/j7vvdfQ2wKny+RonL3d9w933hw3cJ7sgXa3XZX0fzWeBVd9/p7sXAq8CYOMV1I/B0A732Mbn7WwT3MjmascBjHngXyDWzU4jt/jpuXO7+r/B1ofHeX3XZX0dzMu/Nho6rMd9fW9x9Ybi8G1gBdKnVLWbvsZaeILoAGyIeb+TInVvdx90rgRIgv47bxjKuSFMJviEckmZmhWb2rpld3UAx1Seu68Kh7GwzO3Rb2Saxv8JDcb2A1yNWx2p/1cXRYo/l/qqv2u8vB/5qZgvMbFoc4jnLzBab2ctmNiBc1yT2l5llEHzIvhCxulH2lwWHv4cBc2s1xew9FrM7yknDMLOJQAFwfsTqHu6+ycxOBV43s/fd/eNGCumPwNPuvt/MvkAw+vp0I712XUwAZrv7wYh18dxfTZqZXUiQIM6NWH1uuL86AK+a2QfhN+zGsJDg32uPmV0G/B7o00ivXRdXAv9098jRRsz3l5m1IUhKX3X30oZ87mNp6SOITUC3iMddw3VR+1hwH+wcoKiO28YyLszsYuC7wFXuvv/QenffFP5eDbxJ8K2iUeJy96KIWB4CRtR121jGFWECtYb/MdxfdXG02GO5v+rEzAYT/BuOdfeiQ+sj9tc24EUa7tDqcbl7qbvvCZdfApLNrB1NYH+FjvX+isn+MrNkguTwpLv/LkqX2L3HYjGx0lR+CEZIqwkOORya2BpQq8+XqTlJ/Vy4PICak9SrabhJ6rrENYxgUq5PrfV5QGq43A74iAaarKtjXKdELF8DvOuHJ8TWhPHlhcttGyuusF9fgglDa4z9FfEaPTn6pOvl1JxAnBfr/VXHuLoTzKudXWt9JpAVsfwvYEwjxtXp0L8fwQft+nDf1ek9EKu4wvYcgnmKzMbaX+Hf/hhw7zH6xOw91mA7t6n+EMzwf0jwYfvdcN3dBN/KAdKA58P/LPOAUyO2/W643Urg0kaO6zVgK7Ao/JkTrj8beD/8D/I+MLWR4/ofYFn4+m8AfSO2vSXcj6uAmxszrvDxXcCPa20X6/31NLAFqCA4xjsVuB24PWw34P4w7veBgkbaX8eL6yGgOOL9VRiuPzXcV4vDf+fvNnJcd0S8v94lIoFFew80VlxhnykEJ65Ebhfr/XUuwRzHkoh/q8sa6z2mUhsiIhJVS5+DEBGRE6QEISIiUSlBiIhIVEoQIiISlRKEiIhEpQQh0gSEVUz/FO84RCIpQYiISFRKECL1YGYTzWxeWPv/QTNLNLM9FtyPYpkF9+5oH/YdGhYIXGJmL5pZXri+t5m9FhakW2hmp4VP3yYsgPiBmT0ZVhUWiRslCJE6MrN+wHjgHHcfChwEbiIosVDo7gOAvwM/CDd5DPiWuw8muML10PongfvdfQjBld5bwvXDgK8S3IvkVOCcGP9JIsekaq4idXcRQXHC+eGX+3RgG1AFPBv2eQL4nZnlALnu/vdw/aPA82aWBXRx9xcB3L0cIHy+ee6+MXy8iKA20Nsx/6tEjkIJQqTuDHjU3b9dY6XZf9bqd6L1a/ZHLB9E/z8lznSISaTu/gaMC+v+Y2ZtwxsUJQDjwj6fA9529xKg2MzOC9dPAv7uwV3BNh66cZEF90TPaMw/QqSu9A1FpI7cfbmZfY/g7mEJBJU/vwzsBUaFbdsI5ikAJgMPhAlgNXBzuH4S8KCZ3R0+x/WN+GeI1JmquYqcJDPb4+5t4h2HSEPTISYREYlKIwgREYlKIwgREYlKCUJERKJSghARkaiUIEREJColCBERier/A8uI/bquNgkRAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set_style(\"darkgrid\")\n",
    "epoch_f1s = plt.plot(metrics.f1_scores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dir = '../models/model_CWI_full.h5'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(path_dir)  # creates a HDF5 file 'model_CWI_full.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cwi = load_model(path_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.functional.Functional at 0x1f7dad41220>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cwi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_ = set(stopwords.words('english'))\n",
    "def cleaner(word):\n",
    "  #Remove links\n",
    "  word = re.sub(r'((http|https)\\:\\/\\/)?[a-zA-Z0-9\\.\\/\\?\\:@\\-_=#]+\\.([a-zA-Z]){2,6}([a-zA-Z0-9\\.\\&\\/\\?\\:@\\-_=#])*', \n",
    "                '', word, flags=re.MULTILINE)\n",
    "  word = re.sub('[\\W]', ' ', word)\n",
    "  word = re.sub('[^a-zA-Z]', ' ', word)\n",
    "  return word.lower().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_input(input_text):\n",
    "  input_text = cleaner(input_text)\n",
    "  clean_text = []\n",
    "  index_list =[]\n",
    "  input_token = []\n",
    "  index_list_zipf = []\n",
    "  for i, word in enumerate(input_text.split()):\n",
    "    if word in word2index:\n",
    "      clean_text.append(word)\n",
    "      input_token.append(word2index[word])\n",
    "    else:\n",
    "      index_list.append(i)\n",
    "  input_padded = pad_sequences(maxlen=sent_max_length, sequences=[input_token], padding=\"post\", value=0)\n",
    "  return input_padded, index_list, len(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_missing_word(pred_binary, index_list, len_list):\n",
    "  list_cwi_predictions = list(pred_binary[0][:len_list])\n",
    "  for i in index_list:\n",
    "    list_cwi_predictions.insert(i, 0)\n",
    "  return list_cwi_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 1024, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 1024)\n",
       "      (token_type_embeddings): Embedding(2, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (12): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (13): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (14): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (15): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (16): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (17): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (18): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (19): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (20): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (21): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (22): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (23): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=1024, out_features=30522, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_model = 'bert-large-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model)\n",
    "model = BertForMaskedLM.from_pretrained(bert_model)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_candidates(input_text, list_cwi_predictions, numb_predictions_displayed = 10):\n",
    "  list_candidates_bert = []\n",
    "  for word,pred  in zip(input_text.split(), list_cwi_predictions):\n",
    "    if (pred and (pos_tag([word])[0][1] in ['NNS', 'NN', 'VBP', 'RB', 'VBG','VBD' ]))  or (zipf_frequency(word, 'en')) <3.1:\n",
    "      replace_word_mask = input_text.replace(word, '[MASK]')\n",
    "      text = f'[CLS]{replace_word_mask} [SEP] {input_text} [SEP] '\n",
    "      tokenized_text = tokenizer.tokenize(text)\n",
    "      masked_index = [i for i, x in enumerate(tokenized_text) if x == '[MASK]'][0]\n",
    "      indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "      segments_ids = [0]*len(tokenized_text)\n",
    "      tokens_tensor = torch.tensor([indexed_tokens])\n",
    "      segments_tensors = torch.tensor([segments_ids])\n",
    "      # Predict all tokens\n",
    "      with torch.no_grad():\n",
    "          outputs = model(tokens_tensor, token_type_ids=segments_tensors)\n",
    "          predictions = outputs[0][0][masked_index]\n",
    "      predicted_ids = torch.argsort(predictions, descending=True)[:numb_predictions_displayed]\n",
    "      predicted_tokens = tokenizer.convert_ids_to_tokens(list(predicted_ids))\n",
    "      list_candidates_bert.append((word, predicted_tokens))\n",
    "  return list_candidates_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_texts = [ \n",
    " 'The Risk That Students Could Arrive at School With the Coronavirus As schools grapple with how to reopen, new estimates show that large parts of the country would probably see infected students if classrooms opened now.',\n",
    " 'How a photograph of a young man cradling his dying friend sent me on a journey across India.',\n",
    " 'Pro-democracy parties, which had hoped to ride widespread discontent to big gains, saw the yearlong delay as an attempt to thwart them.',\n",
    " 'Night after night, calm gave way to chaos. See what happened between the protesters and the federal agents.',\n",
    " 'Contact Tracing Is Failing in Many States. Here is Why. Inadequate testing and protracted delays in producing results have crippled tracking and hampered efforts to contain major outbreaks.',\n",
    " 'After an initial decrease in the youth detention population, the rate of release has slowed, and the gap between white youth and Black youth has grown.'\n",
    " 'A laboratory experiment hints at some of the ways the virus might elude antibody treatments. Combining therapies could help, experts said.',\n",
    " 'Though I may not be here with you, I urge you to answer the highest calling of your heart and stand up for what you truly believe.',\n",
    " 'The research does not prove that infected children are contagious, but it should influence the debate about reopening schools, some experts said.',\n",
    " 'Dropping antibody counts are not a sign that our immune system is failing against the coronavirus, nor an omen that we can not develop a viable vaccine.',\n",
    " 'The Senate majority leader has said he will not approve a stimulus package without a “liability shield,” but top White House officials say they do not see it as essential.',\n",
    " 'Campaign efforts to refocus come as the president continues to push divisive messages that have frustrated his own party.'\n",
    "] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:  The Risk That Students Could Arrive at School With the Coronavirus As schools grapple with how to reopen, new estimates show that large parts of the country would probably see infected students if classrooms opened now.\n",
      "Simplified text: The Risk That Students Could Arrive at School With the Coronavirus As schools deal with how to open new numbers show that large parts of the country would probably see infected students if they opened now. \n",
      "\n",
      "Original text:  How a photograph of a young man cradling his dying friend sent me on a journey across India.\n",
      "Simplified text: How a shot of a young man and his dying friend sent me on a tour across India. \n",
      "\n",
      "Original text:  Pro-democracy parties, which had hoped to ride widespread discontent to big gains, saw the yearlong delay as an attempt to thwart them.\n",
      "Simplified text: Pro-democracy parties, which had hoped to ride widespread opposition to big gains, saw the year delay as an attempt to stop while \n",
      "\n",
      "Original text:  Night after night, calm gave way to chaos. See what happened between the protesters and the federal agents.\n",
      "Simplified text: Night after night, order gave way to chaos See what happened between the people and the federal agents. \n",
      "\n",
      "Original text:  Contact Tracing Is Failing in Many States. Here is Why. Inadequate testing and protracted delays in producing results have crippled tracking and hampered efforts to contain major outbreaks.\n",
      "Simplified text: Contact dating Is failed in Many States. Here is Why. poor testing and protracted delay in producing results have crippled tracking and hampered efforts to contain major events \n",
      "\n",
      "Original text:  After an initial decrease in the youth detention population, the rate of release has slowed, and the gap between white youth and Black youth has grown.A laboratory experiment hints at some of the ways the virus might elude antibody treatments. Combining therapies could help, experts said.\n",
      "Simplified text: After an initial fall in the youth prison population, the rate of release has increased and the gap between white youth and Black youth has the laboratory research hints at some of the ways the virus might reach antibody and Combining therapies could help, experts said. \n",
      "\n",
      "Original text:  Though I may not be here with you, I urge you to answer the highest calling of your heart and stand up for what you truly believe.\n",
      "Simplified text: Though I may not be here with you, I ask you to answer the highest calling of your heart and stand up for what you truly believe. \n",
      "\n",
      "Original text:  The research does not prove that infected children are contagious, but it should influence the debate about reopening schools, some experts said.\n",
      "Simplified text: The work does not show that infected children are sick but it should change the question about reopening schools, some experts said. \n",
      "\n",
      "Original text:  Dropping antibody counts are not a sign that our immune system is failing against the coronavirus, nor an omen that we can not develop a viable vaccine.\n",
      "Simplified text: Dropping blood counts are not a sign that our immune system is failed against the coronavirus, nor an omen that we can not develop a viable vaccine. \n",
      "\n",
      "Original text:  The Senate majority leader has said he will not approve a stimulus package without a “liability shield,” but top White House officials say they do not see it as essential.\n",
      "Simplified text: The Senate majority leader has said he will not approve a tax package without a risk defense but top White House officials say they do not see it as possible \n",
      "\n",
      "Original text:  Campaign efforts to refocus come as the president continues to push divisive messages that have frustrated his own party.\n",
      "Simplified text: the efforts to lead come as the president continues to push the messages that have frustrated his own party. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for input_text in list_texts:\n",
    "  new_text = input_text\n",
    "  input_padded, index_list, len_list = process_input(input_text)\n",
    "  pred_cwi = model_cwi.predict(input_padded)\n",
    "  pred_cwi_binary = np.argmax(pred_cwi, axis = 2)\n",
    "  complete_cwi_predictions = complete_missing_word(pred_cwi_binary, index_list, len_list)\n",
    "  bert_candidates =   get_bert_candidates(input_text, complete_cwi_predictions)\n",
    "  for word_to_replace, l_candidates in bert_candidates:\n",
    "    tuples_word_zipf = []\n",
    "    for w in l_candidates:\n",
    "      if w.isalpha():\n",
    "        tuples_word_zipf.append((w, zipf_frequency(w, 'en')))\n",
    "    tuples_word_zipf = sorted(tuples_word_zipf, key = lambda x: x[1], reverse=True)\n",
    "    new_text = re.sub(word_to_replace, tuples_word_zipf[0][0], new_text) \n",
    "  print(\"Original text: \", input_text )\n",
    "  print(\"Simplified text:\", new_text, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "254e3b970a7dd062895c9e26f37fa2538908b1e74b276da92edb09857b48a424"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit ('simplification': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
