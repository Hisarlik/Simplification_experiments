{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replicated from : https://medium.com/@armandj.olivares/how-to-use-bert-for-lexical-simplification-6edbf5a4d15e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "from functools import lru_cache\n",
    "from collections import Counter\n",
    "from collections import namedtuple\n",
    "\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "from nltk import word_tokenize\n",
    "#nltk.download('brown')\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.callbacks\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from wordfreq import zipf_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset = namedtuple('Dataset', 'name, train, test')\n",
    "ModelTuple = namedtuple('Model', 'type, name, dimension, corpus, model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas config\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_colwidth', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAIN_PATH_DATASET = \"../../../data/cwi_shared_dataset/traindevset/english/\"\n",
    "MAIN_PATH_EMBEDDINGS= '../../../data/glove.6B/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "genres = ['Wikipedia', 'WikiNews', 'News']\n",
    "datasets = ['Train', 'Dev']\n",
    "columns = ['id', 'sentence', \"start\", \"end\", \"target\", \n",
    "           \"nat\", \"non_nat\", \"nat_marked\", \"non_nat_marked\", \"binary\", \"prob\"]\n",
    "\n",
    "\n",
    "datasets = [Dataset('Wikipedia', 'Train', 'Dev'),\n",
    "            Dataset('WikiNews', 'Train', 'Dev'),\n",
    "            Dataset('News', 'Train', 'Dev')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_df(path):\n",
    "    df = pd.read_csv(path, header=None, sep = \"\\t\")\n",
    "    df.columns = columns\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [Dataset(d.name, load_df(MAIN_PATH_DATASET + d.name + '_' + d.train + '.tsv'),\n",
    "                            load_df(MAIN_PATH_DATASET + d.name + '_' + d.test + '.tsv'))\n",
    "                            for d in datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>target</th>\n",
       "      <th>nat</th>\n",
       "      <th>non_nat</th>\n",
       "      <th>nat_marked</th>\n",
       "      <th>non_nat_marked</th>\n",
       "      <th>binary</th>\n",
       "      <th>prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>Normally , the land will be passed down to future generations in a way that recognizes the community 's traditional connection to that country .</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>Normally</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>Normally , the land will be passed down to future generations in a way that recognizes the community 's traditional connection to that country .</td>\n",
       "      <td>28</td>\n",
       "      <td>34</td>\n",
       "      <td>passed</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>Normally , the land will be passed down to future generations in a way that recognizes the community 's traditional connection to that country .</td>\n",
       "      <td>15</td>\n",
       "      <td>19</td>\n",
       "      <td>land</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>Normally , the land will be passed down to future generations in a way that recognizes the community 's traditional connection to that country .</td>\n",
       "      <td>43</td>\n",
       "      <td>49</td>\n",
       "      <td>future</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>Normally , the land will be passed down to future generations in a way that recognizes the community 's traditional connection to that country .</td>\n",
       "      <td>43</td>\n",
       "      <td>61</td>\n",
       "      <td>future generations</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               id  \\\n",
       "0  3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "1  3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "2  3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "3  3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "4  3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "\n",
       "                                                                                                                                           sentence  \\\n",
       "0  Normally , the land will be passed down to future generations in a way that recognizes the community 's traditional connection to that country .   \n",
       "1  Normally , the land will be passed down to future generations in a way that recognizes the community 's traditional connection to that country .   \n",
       "2  Normally , the land will be passed down to future generations in a way that recognizes the community 's traditional connection to that country .   \n",
       "3  Normally , the land will be passed down to future generations in a way that recognizes the community 's traditional connection to that country .   \n",
       "4  Normally , the land will be passed down to future generations in a way that recognizes the community 's traditional connection to that country .   \n",
       "\n",
       "   start  end              target  nat  non_nat  nat_marked  non_nat_marked  \\\n",
       "0      0    8            Normally   10       10           0               1   \n",
       "1     28   34              passed   10       10           0               1   \n",
       "2     15   19                land   10       10           0               0   \n",
       "3     43   49              future   10       10           1               0   \n",
       "4     43   61  future generations   10       10           1               2   \n",
       "\n",
       "   binary  prob  \n",
       "0       1  0.05  \n",
       "1       1  0.05  \n",
       "2       0  0.00  \n",
       "3       1  0.05  \n",
       "4       1  0.15  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[0].train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Antonio\\AppData\\Local\\Temp/ipykernel_35864/1946840910.py:9: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n",
      "  glove2word2vec(glove_file, tmp_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model : glove.6B.300d.txt\n",
      "[Model(type='glove', name='glove.6B.300d.txt', dimension=300, corpus='wikipedia+gigaword5', model=<gensim.models.keyedvectors.KeyedVectors object at 0x0000020E200D2520>)]\n"
     ]
    }
   ],
   "source": [
    "# load embedding model\n",
    "glove_models = []\n",
    "\n",
    "glove_defs = [ ModelTuple('glove', 'glove.6B.300d.txt', 300, 'wikipedia+gigaword5', None)]\n",
    "              \n",
    "for model in glove_defs:\n",
    "    glove_file = MAIN_PATH_EMBEDDINGS + model.name\n",
    "    tmp_file = get_tmpfile(model.name + '-temp')\n",
    "    glove2word2vec(glove_file, tmp_file)\n",
    "    vecs = KeyedVectors.load_word2vec_format(tmp_file)\n",
    "    glove_models.append(ModelTuple(model.type, model.name, model.dimension, model.corpus, vecs))\n",
    "    print('load model : {}'.format(model.name))\n",
    "    \n",
    "print(glove_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49815\n"
     ]
    }
   ],
   "source": [
    "wordlist_lowercased = set(i.lower() for i in brown.words())\n",
    "print (len(wordlist_lowercased))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets.append(Dataset('train_all_test_wiki', \n",
    "        datasets[0].train.append(datasets[1].train).append(datasets[2].train), datasets[0].test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Rows train : 27299\n",
      "# Rows test : 694\n",
      "# Rows dataset : 27993\n",
      "# Sents train : 1988\n",
      "# Sents test : 53\n",
      "# Sents dataset : 2041\n"
     ]
    }
   ],
   "source": [
    "# Append train and test set\n",
    "dataset_sel = datasets[3]\n",
    "train_num_rows = dataset_sel.train.shape[0]\n",
    "train_num_sents = len(list(set(dataset_sel.train.sentence.values.tolist())))\n",
    "\n",
    "test_num_rows = dataset_sel.test.shape[0]\n",
    "test_num_sents = len(list(set(dataset_sel.test.sentence.values.tolist())))\n",
    "\n",
    "dataset = dataset_sel.train.append(dataset_sel.test)\n",
    "dataset['sent_id'] = dataset.groupby('sentence').ngroup()\n",
    "dataset_num_rows = dataset.shape[0]\n",
    "dataset_num_sents = len(list(set(dataset.sentence.values.tolist())))\n",
    "\n",
    "print('# Rows train : {}'.format(train_num_rows))\n",
    "print('# Rows test : {}'.format(test_num_rows))\n",
    "print('# Rows dataset : {}'.format(dataset_num_rows))\n",
    "\n",
    "print('# Sents train : {}'.format(train_num_sents))\n",
    "print('# Sents test : {}'.format(test_num_sents))\n",
    "print('# Sents dataset : {}'.format(dataset_num_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbl = dict.fromkeys(i for i in range(sys.maxunicode)\n",
    "                      if unicodedata.category(chr(i)).startswith('P'))\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(tbl)\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def all_tokens_with_index(context):\n",
    "    curr_pos = 0\n",
    "    targets = []\n",
    "    j = 0\n",
    "    w = 0\n",
    "    curr_split = ''\n",
    "    ctx_split = context.split()\n",
    "    whitespaces = re.findall('\\s+', context)\n",
    "    num_whitespaces = [len(token) for token in whitespaces]\n",
    "    num_whitespaces.append(1)\n",
    "    tokens = word_tokenize(context)\n",
    "    tokens = ['\"' if token not in context else token for token in tokens]\n",
    "    for index, token in enumerate(tokens, 1):\n",
    "        targets.append((token, index, curr_pos, (curr_pos + len(token))))\n",
    "        curr_pos += len(token)\n",
    "        curr_split += token\n",
    "        if ctx_split[j] == curr_split:\n",
    "            curr_pos += num_whitespaces[w]\n",
    "            j += 1\n",
    "            w += 1\n",
    "            curr_split = ''\n",
    "    return [val for val in targets if val[0] != '\"']\n",
    "\n",
    "def build_vocabulary(sentences, embedding_model, dimension):\n",
    "    all_words = [tpl[0] for sentence in sentences for tpl in sentence['seq']] + list(wordlist_lowercased)\n",
    "    print('# Words : {}'.format(len(all_words)))\n",
    "    counter = Counter(all_words)\n",
    "    vocab_size = len(counter) + 1\n",
    "    print('# Vocab : {}'.format(vocab_size))\n",
    "    print('# embeding model  : {}'.format(len(embedding_model)))   \n",
    "    word2index = {word : index for index, (word, count) in enumerate(counter.most_common(), 1)}\n",
    "    index2word = {index : word for word, index in word2index.items()}\n",
    "    # +1 required for pad token\n",
    "    embedding_matrix = np.zeros(((vocab_size), dimension))\n",
    "    missing_embed_words = []\n",
    "    i_ = 0\n",
    "    for word, index in word2index.items():\n",
    "        if word in embedding_model.key_to_index:\n",
    "            embedding = embedding_model[word]\n",
    "        else:\n",
    "             i_ +=1\n",
    "             missing_embed_words.append(word)\n",
    "             continue\n",
    "        embedding_matrix[index] = embedding\n",
    "    missing_embed_count = len(missing_embed_words)\n",
    "    print('# Words missing embedding : {}'.format(missing_embed_count))\n",
    "    print('Embedding shape : {}'.format(embedding_matrix.shape))\n",
    "    print(\"i: \", i_ )\n",
    "    return word2index, index2word, embedding_matrix\n",
    "\n",
    "def forward_transformation(dataframe, lowercase = True, filter_punc = True, filtering = \"a132\"):\n",
    "    grouped = dataframe.groupby('sentence').apply(lambda row : \n",
    "                        {'sent_id' : list(set(row['sent_id']))[0],\n",
    "                         'sentence' : list(set(row['sentence']))[0], \n",
    "                         'tags': [tag for tag in zip(row['target'], \n",
    "                            row['start'], row['end'], row['binary'], row['prob'])]})\n",
    "    sentences = []\n",
    "    for vals in grouped:\n",
    "        sent_id = vals['sent_id']\n",
    "        sentence = vals['sentence']\n",
    "        tags = vals['tags']\n",
    "        tags_without_labels = [(word, start, end) for word, start, end, binary, prob in tags]\n",
    "        all_tokens = all_tokens_with_index(sentence)\n",
    "        sent_repr = [(word, start, end, tags[tags_without_labels.index((word, start, end))][3],\n",
    "                     tags[tags_without_labels.index((word, start, end))][4])\n",
    "           if (word, start, end) in tags_without_labels \n",
    "          else (word, start, end, 0, 0.0) for word, index, start, end in all_tokens]\n",
    "        if lowercase:\n",
    "            sent_repr = [(word.lower(), start, end, binary, prob) \n",
    "                         for word, start, end, binary, prob in sent_repr]\n",
    "        if filter_punc:\n",
    "            sent_repr = list(filter(lambda vals : remove_punctuation(vals[0]), sent_repr))\n",
    "        if filtering:\n",
    "            sent_repr = list(filter(lambda vals : vals[0] != \"'s\", sent_repr))\n",
    "            sent_repr = list(filter(lambda vals : vals[0] != \"``\", sent_repr))\n",
    "        sentences.append({'sent_id' : sent_id, 'sentence' : sentence, 'seq' : sent_repr})\n",
    "    return sentences\n",
    "\n",
    "def split_sentence_seqs(sentences):\n",
    "    words, start_end, binary, prob = [], [], [] ,[]\n",
    "    for sent in sentences:\n",
    "        sequence = sent['seq']\n",
    "        curr_w, curr_se, curr_b, curr_p = map(list, zip(*[(vals[0], \n",
    "            (vals[1], vals[2]), vals[3], vals[4]) for vals in sequence]))\n",
    "        words.append(curr_w)\n",
    "        start_end.append(curr_se)\n",
    "        binary.append(curr_b)\n",
    "        prob.append(curr_p)\n",
    "    return words, start_end, binary, prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = forward_transformation(dataset)\n",
    "train_sentences = sentences[:train_num_sents]\n",
    "test_sentences = sentences[train_num_sents:]\n",
    "words, start_end, binary, prob = split_sentence_seqs(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Words : 96437\n",
      "# Vocab : 52455\n",
      "# embeding model  : 400000\n",
      "# Words missing embedding : 9834\n",
      "Embedding shape : (52455, 300)\n",
      "i:  9834\n"
     ]
    }
   ],
   "source": [
    "embedding_model = glove_models[0].model\n",
    "dimension = embedding_model.vector_size\n",
    "word2index, index2word, embedding = build_vocabulary(sentences, embedding_model, dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length sentence : 103\n"
     ]
    }
   ],
   "source": [
    "\n",
    "words_with_indices = [[word2index[word] for word in sent] for sent in words]\n",
    "sent_lens = [len(sentence['seq']) for sentence in sentences]\n",
    "sent_max_length = np.max(sent_lens)\n",
    "print('Max length sentence : {}'.format(sent_max_length))\n",
    "\n",
    "\n",
    "words_padded = pad_sequences(maxlen=sent_max_length, sequences=words_with_indices, padding=\"post\", value=0)\n",
    "binary_padded = pad_sequences(maxlen=sent_max_length, sequences=binary, padding=\"post\", value=0)\n",
    "prob_padded = pad_sequences(maxlen=sent_max_length, sequences=prob, padding=\"post\", value=0, dtype=\"float\")\n",
    "\n",
    "binary_padded_categorical = [to_categorical(clazz, num_classes=2) for clazz in binary_padded]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length sentence : 103\n"
     ]
    }
   ],
   "source": [
    "words_with_indices = [[word2index[word] for word in sent] for sent in words]\n",
    "sent_lens = [len(sentence['seq']) for sentence in sentences]\n",
    "sent_max_length = np.max(sent_lens)\n",
    "print('Max length sentence : {}'.format(sent_max_length))\n",
    "\n",
    "words_padded = pad_sequences(maxlen=sent_max_length, sequences=words_with_indices, padding=\"post\", value=0)\n",
    "binary_padded = pad_sequences(maxlen=sent_max_length, sequences=binary, padding=\"post\", value=0)\n",
    "prob_padded = pad_sequences(maxlen=sent_max_length, sequences=prob, padding=\"post\", value=0, dtype=\"float\")\n",
    "\n",
    "binary_padded_categorical = [to_categorical(clazz, num_classes=2) for clazz in binary_padded]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set length : 1988\n",
      "Test set length : 53\n"
     ]
    }
   ],
   "source": [
    "# (1) Training set\n",
    "train_words_padded = words_padded[:train_num_sents]\n",
    "train_binary_padded = binary_padded[:train_num_sents]\n",
    "train_binary_padded_categorical = binary_padded_categorical[:train_num_sents]\n",
    "train_prob_padded = prob_padded[:train_num_sents]\n",
    "train_start_end = start_end[:train_num_sents]\n",
    "\n",
    "# (2) Test set\n",
    "test_words_padded = words_padded[train_num_sents:]\n",
    "test_binary_padded = binary_padded[train_num_sents:]\n",
    "test_binary_padded_categorical = binary_padded_categorical[train_num_sents:]\n",
    "test_prob_padded = prob_padded[train_num_sents:]\n",
    "test_start_end = start_end[train_num_sents:]\n",
    "\n",
    "print('Training set length : {}'.format(len(train_words_padded)))\n",
    "print('Test set length : {}'.format(len(test_words_padded)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics(tensorflow.keras.callbacks.Callback):\n",
    "    def __init__(self, validation_data):\n",
    "        self.f1_scores = []\n",
    "        self.validation_data = validation_data\n",
    "        \n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        predict = np.asarray(self.model.predict(self.validation_data[0]))\n",
    "        targ = self.validation_data[1]\n",
    "        targ = np.array(targ)\n",
    "        shape = targ.shape\n",
    "        targ = targ.reshape((shape[0]*shape[1], shape[2]))\n",
    "        targ = np.argmax(targ, axis = 1)\n",
    "        predict = predict.reshape((shape[0]*shape[1]), shape[2])\n",
    "        predict = np.argmax(predict, axis = 1)\n",
    "        self.f1s=f1_score(targ, predict)\n",
    "        print(\"\\nF1 Score:\")\n",
    "        print(f1_score(targ, np.ones(shape[0]*shape[1])))\n",
    "        self.f1_scores.append(self.f1s)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1988, 103), (1988, 103, 2))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_words_padded.shape, np.array(train_binary_padded_categorical).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((52455, 300), 103)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.shape, sent_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 103)]             0         \n",
      "                                                                 \n",
      " embedding_1 (Embedding)     (None, 103, 300)          15736500  \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 103, 300)          0         \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 103, 300)         541200    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " time_distributed_1 (TimeDis  (None, 103, 2)           602       \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16,278,302\n",
      "Trainable params: 16,278,302\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = embedding.shape[0]\n",
    "dimension = embedding.shape[1]\n",
    "\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "\n",
    "in_seq = Input(shape=(sent_max_length,))\n",
    "embed = Embedding(input_dim=vocab_size, output_dim=dimension, \\\n",
    "                  weights=[embedding], input_length=sent_max_length)(in_seq)\n",
    "drop = Dropout(0.1)(embed)\n",
    "lstm = Bidirectional(LSTM(units=150, return_sequences=True, recurrent_dropout=0.1))(drop)\n",
    "out = TimeDistributed(Dense(2, activation=\"softmax\"))(lstm) \n",
    "\n",
    "model = Model(in_seq, out)\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "model.summary()\n",
    "\n",
    "metrics = Metrics((test_words_padded, np.array(test_binary_padded_categorical)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "199/199 [==============================] - ETA: 0s - loss: 0.0868 - accuracy: 0.9691\n",
      "F1 Score:\n",
      "0.05660377358490566\n",
      "199/199 [==============================] - 79s 379ms/step - loss: 0.0868 - accuracy: 0.9691 - val_loss: 0.0470 - val_accuracy: 0.9780\n",
      "Epoch 2/3\n",
      "199/199 [==============================] - ETA: 0s - loss: 0.0400 - accuracy: 0.9830\n",
      "F1 Score:\n",
      "0.05660377358490566\n",
      "199/199 [==============================] - 76s 380ms/step - loss: 0.0400 - accuracy: 0.9830 - val_loss: 0.0437 - val_accuracy: 0.9808\n",
      "Epoch 3/3\n",
      "199/199 [==============================] - ETA: 0s - loss: 0.0301 - accuracy: 0.9873\n",
      "F1 Score:\n",
      "0.05660377358490566\n",
      "199/199 [==============================] - 78s 391ms/step - loss: 0.0301 - accuracy: 0.9873 - val_loss: 0.0462 - val_accuracy: 0.9808\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_words_padded, np.array(train_binary_padded_categorical), batch_size=10, \n",
    "                    epochs=3, validation_data = (test_words_padded, np.array(test_binary_padded_categorical)), \n",
    "                    verbose=1, callbacks=[metrics])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set_style(\"darkgrid\")\n",
    "epoch_f1s = plt.plot(metrics.f1_scores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dir = '../../../models/lexical/bertls/model_CWI_full.h5'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(path_dir)  # creates a HDF5 file 'model_CWI_full.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cwi = load_model(path_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.functional.Functional at 0x20e19eddc70>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cwi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_ = set(stopwords.words('english'))\n",
    "def cleaner(word):\n",
    "  #Remove links\n",
    "  word = re.sub(r'((http|https)\\:\\/\\/)?[a-zA-Z0-9\\.\\/\\?\\:@\\-_=#]+\\.([a-zA-Z]){2,6}([a-zA-Z0-9\\.\\&\\/\\?\\:@\\-_=#])*', \n",
    "                '', word, flags=re.MULTILINE)\n",
    "  word = re.sub('[\\W]', ' ', word)\n",
    "  word = re.sub('[^a-zA-Z]', ' ', word)\n",
    "  return word.lower().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_input(input_text):\n",
    "  input_text = cleaner(input_text)\n",
    "  clean_text = []\n",
    "  index_list =[]\n",
    "  input_token = []\n",
    "  index_list_zipf = []\n",
    "  for i, word in enumerate(input_text.split()):\n",
    "    if word in word2index:\n",
    "      clean_text.append(word)\n",
    "      input_token.append(word2index[word])\n",
    "    else:\n",
    "      index_list.append(i)\n",
    "  input_padded = pad_sequences(maxlen=sent_max_length, sequences=[input_token], padding=\"post\", value=0)\n",
    "  return input_padded, index_list, len(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_missing_word(pred_binary, index_list, len_list):\n",
    "  list_cwi_predictions = list(pred_binary[0][:len_list])\n",
    "  for i in index_list:\n",
    "    list_cwi_predictions.insert(i, 0)\n",
    "  return list_cwi_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 1024, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 1024)\n",
       "      (token_type_embeddings): Embedding(2, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (12): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (13): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (14): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (15): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (16): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (17): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (18): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (19): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (20): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (21): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (22): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (23): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=1024, out_features=30522, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_model = 'bert-large-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model)\n",
    "model = BertForMaskedLM.from_pretrained(bert_model)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_candidates(input_text, list_cwi_predictions, numb_predictions_displayed = 10):\n",
    "  list_candidates_bert = []\n",
    "  for word,pred  in zip(input_text.split(), list_cwi_predictions):\n",
    "    if (pred and (pos_tag([word])[0][1] in ['NNS', 'NN', 'VBP', 'RB', 'VBG','VBD' ]))  or (zipf_frequency(word, 'en')) <3.1:\n",
    "      replace_word_mask = input_text.replace(word, '[MASK]')\n",
    "      text = f'[CLS]{replace_word_mask} [SEP] {input_text} [SEP] '\n",
    "      tokenized_text = tokenizer.tokenize(text)\n",
    "      masked_index = [i for i, x in enumerate(tokenized_text) if x == '[MASK]'][0]\n",
    "      indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "      segments_ids = [0]*len(tokenized_text)\n",
    "      tokens_tensor = torch.tensor([indexed_tokens])\n",
    "      segments_tensors = torch.tensor([segments_ids])\n",
    "      # Predict all tokens\n",
    "      with torch.no_grad():\n",
    "          outputs = model(tokens_tensor, token_type_ids=segments_tensors)\n",
    "          predictions = outputs[0][0][masked_index]\n",
    "      predicted_ids = torch.argsort(predictions, descending=True)[:numb_predictions_displayed]\n",
    "      predicted_tokens = tokenizer.convert_ids_to_tokens(list(predicted_ids))\n",
    "      list_candidates_bert.append((word, predicted_tokens))\n",
    "  return list_candidates_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_texts = [ \n",
    " 'The Risk That Students Could Arrive at School With the Coronavirus As schools grapple with how to reopen, new estimates show that large parts of the country would probably see infected students if classrooms opened now.',\n",
    " 'How a photograph of a young man cradling his dying friend sent me on a journey across India.',\n",
    " 'Pro-democracy parties, which had hoped to ride widespread discontent to big gains, saw the yearlong delay as an attempt to thwart them.',\n",
    " 'Night after night, calm gave way to chaos. See what happened between the protesters and the federal agents.',\n",
    " 'Contact Tracing Is Failing in Many States. Here is Why. Inadequate testing and protracted delays in producing results have crippled tracking and hampered efforts to contain major outbreaks.',\n",
    " 'After an initial decrease in the youth detention population, the rate of release has slowed, and the gap between white youth and Black youth has grown.'\n",
    " 'A laboratory experiment hints at some of the ways the virus might elude antibody treatments. Combining therapies could help, experts said.',\n",
    " 'Though I may not be here with you, I urge you to answer the highest calling of your heart and stand up for what you truly believe.',\n",
    " 'The research does not prove that infected children are contagious, but it should influence the debate about reopening schools, some experts said.',\n",
    " 'Dropping antibody counts are not a sign that our immune system is failing against the coronavirus, nor an omen that we can not develop a viable vaccine.',\n",
    " 'The Senate majority leader has said he will not approve a stimulus package without a liability shield, but top White House officials say they do not see it as essential.',\n",
    " 'Campaign efforts to refocus come as the president continues to push divisive messages that have frustrated his own party.'\n",
    "] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:  The Risk That Students Could Arrive at School With the Coronavirus As schools grapple with how to reopen, new estimates show that large parts of the country would probably see infected students if classrooms opened now.\n",
      "Simplified text: The Risk That Students Could Arrive at School With the Coronavirus As schools deal with how to open new numbers show that large parts of the country would probably see infected students if they opened now. \n",
      "\n",
      "Original text:  How a photograph of a young man cradling his dying friend sent me on a journey across India.\n",
      "Simplified text: How a shot of a young man and his dying friend sent me on a journey across India. \n",
      "\n",
      "Original text:  Pro-democracy parties, which had hoped to ride widespread discontent to big gains, saw the yearlong delay as an attempt to thwart them.\n",
      "Simplified text: Pro-democracy parties, which had hoped to ride widespread opposition to big gains, see the year delay as an attempt to stop while \n",
      "\n",
      "Original text:  Night after night, calm gave way to chaos. See what happened between the protesters and the federal agents.\n",
      "Simplified text: Night after night, order gave way to chaos See what happened between the people and the federal agent \n",
      "\n",
      "Original text:  Contact Tracing Is Failing in Many States. Here is Why. Inadequate testing and protracted delays in producing results have crippled tracking and hampered efforts to contain major outbreaks.\n",
      "Simplified text: Contact dating Is failed in Many States. Here is Why. poor testing and protracted delay in producing results have crippled tracking and hampered efforts to contain major events \n",
      "\n",
      "Original text:  After an initial decrease in the youth detention population, the rate of release has slowed, and the gap between white youth and Black youth has grown.A laboratory experiment hints at some of the ways the virus might elude antibody treatments. Combining therapies could help, experts said.\n",
      "Simplified text: After an initial fall in the youth prison population, the rate of release has increased and the gap between white youth and Black youth has the laboratory research looks at some of the ways the virus might reach antibody and using them could help, experts said. \n",
      "\n",
      "Original text:  Though I may not be here with you, I urge you to answer the highest calling of your heart and stand up for what you truly believe.\n",
      "Simplified text: Though I may not be here with you, I ask you to answer the highest calling of your heart and stand up for what you truly believe. \n",
      "\n",
      "Original text:  The research does not prove that infected children are contagious, but it should influence the debate about reopening schools, some experts said.\n",
      "Simplified text: The research does not show that infected children are sick but it should change the question about open schools, some experts said. \n",
      "\n",
      "Original text:  Dropping antibody counts are not a sign that our immune system is failing against the coronavirus, nor an omen that we can not develop a viable vaccine.\n",
      "Simplified text: Dropping antibody counts are not a sign that our immune system is failed against the coronavirus, nor an omen that we can not develop a viable vaccine. \n",
      "\n",
      "Original text:  The Senate majority leader has said he will not approve a stimulus package without a liability shield, but top White House officials say they do not see it as essential.\n",
      "Simplified text: The Senate majority leader has said he will not approve a tax package without a risk defense but top White House officials say they do not see it as possible \n",
      "\n",
      "Original text:  Campaign efforts to refocus come as the president continues to push divisive messages that have frustrated his own party.\n",
      "Simplified text: the efforts to lead come as the president continues to push the messages that have frustrated his own party. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for input_text in list_texts:\n",
    "  new_text = input_text\n",
    "  input_padded, index_list, len_list = process_input(input_text)\n",
    "  pred_cwi = model_cwi.predict(input_padded)\n",
    "  pred_cwi_binary = np.argmax(pred_cwi, axis = 2)\n",
    "  complete_cwi_predictions = complete_missing_word(pred_cwi_binary, index_list, len_list)\n",
    "  bert_candidates =   get_bert_candidates(input_text, complete_cwi_predictions)\n",
    "  for word_to_replace, l_candidates in bert_candidates:\n",
    "    tuples_word_zipf = []\n",
    "    for w in l_candidates:\n",
    "      if w.isalpha():\n",
    "        tuples_word_zipf.append((w, zipf_frequency(w, 'en')))\n",
    "    tuples_word_zipf = sorted(tuples_word_zipf, key = lambda x: x[1], reverse=True)\n",
    "    new_text = re.sub(word_to_replace, tuples_word_zipf[0][0], new_text) \n",
    "  print(\"Original text: \", input_text )\n",
    "  print(\"Simplified text:\", new_text, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "254e3b970a7dd062895c9e26f37fa2538908b1e74b276da92edb09857b48a424"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit ('simplification': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
