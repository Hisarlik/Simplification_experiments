{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replicated from : https://medium.com/@armandj.olivares/how-to-use-bert-for-lexical-simplification-6edbf5a4d15e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "from functools import lru_cache\n",
    "from collections import Counter\n",
    "from collections import namedtuple\n",
    "\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "from nltk import word_tokenize\n",
    "#nltk.download('brown')\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.callbacks\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from wordfreq import zipf_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset = namedtuple('Dataset', 'name, train, test')\n",
    "ModelTuple = namedtuple('Model', 'type, name, dimension, corpus, model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas config\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_colwidth', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAIN_PATH_DATASET = \"../../../../data/cwi_shared_dataset/traindevset/english/\"\n",
    "MAIN_PATH_EMBEDDINGS= '../../../../data/glove.6B/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genres = ['Wikipedia', 'WikiNews', 'News']\n",
    "datasets = ['Train', 'Dev']\n",
    "columns = ['id', 'sentence', \"start\", \"end\", \"target\", \n",
    "           \"nat\", \"non_nat\", \"nat_marked\", \"non_nat_marked\", \"binary\", \"prob\"]\n",
    "\n",
    "\n",
    "datasets = [Dataset('Wikipedia', 'Train', 'Dev'),\n",
    "            Dataset('WikiNews', 'Train', 'Dev'),\n",
    "            Dataset('News', 'Train', 'Dev')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_df(path):\n",
    "    df = pd.read_csv(path, header=None, sep = \"\\t\")\n",
    "    df.columns = columns\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [Dataset(d.name, load_df(MAIN_PATH_DATASET + d.name + '_' + d.train + '.tsv'),\n",
    "                            load_df(MAIN_PATH_DATASET + d.name + '_' + d.test + '.tsv'))\n",
    "                            for d in datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets[0].train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load embedding model\n",
    "glove_models = []\n",
    "\n",
    "glove_defs = [ ModelTuple('glove', 'glove.6B.300d.txt', 300, 'wikipedia+gigaword5', None)]\n",
    "              \n",
    "for model in glove_defs:\n",
    "    glove_file = MAIN_PATH_EMBEDDINGS + model.name\n",
    "    tmp_file = get_tmpfile(model.name + '-temp')\n",
    "    glove2word2vec(glove_file, tmp_file)\n",
    "    vecs = KeyedVectors.load_word2vec_format(tmp_file)\n",
    "    glove_models.append(ModelTuple(model.type, model.name, model.dimension, model.corpus, vecs))\n",
    "    print('load model : {}'.format(model.name))\n",
    "    \n",
    "print(glove_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordlist_lowercased = set(i.lower() for i in brown.words())\n",
    "print (len(wordlist_lowercased))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets.append(Dataset('train_all_test_wiki', \n",
    "        datasets[0].train.append(datasets[1].train).append(datasets[2].train), datasets[0].test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append train and test set\n",
    "dataset_sel = datasets[3]\n",
    "train_num_rows = dataset_sel.train.shape[0]\n",
    "train_num_sents = len(list(set(dataset_sel.train.sentence.values.tolist())))\n",
    "\n",
    "test_num_rows = dataset_sel.test.shape[0]\n",
    "test_num_sents = len(list(set(dataset_sel.test.sentence.values.tolist())))\n",
    "\n",
    "dataset = dataset_sel.train.append(dataset_sel.test)\n",
    "dataset['sent_id'] = dataset.groupby('sentence').ngroup()\n",
    "dataset_num_rows = dataset.shape[0]\n",
    "dataset_num_sents = len(list(set(dataset.sentence.values.tolist())))\n",
    "\n",
    "print('# Rows train : {}'.format(train_num_rows))\n",
    "print('# Rows test : {}'.format(test_num_rows))\n",
    "print('# Rows dataset : {}'.format(dataset_num_rows))\n",
    "\n",
    "print('# Sents train : {}'.format(train_num_sents))\n",
    "print('# Sents test : {}'.format(test_num_sents))\n",
    "print('# Sents dataset : {}'.format(dataset_num_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbl = dict.fromkeys(i for i in range(sys.maxunicode)\n",
    "                      if unicodedata.category(chr(i)).startswith('P'))\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(tbl)\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def all_tokens_with_index(context):\n",
    "    curr_pos = 0\n",
    "    targets = []\n",
    "    j = 0\n",
    "    w = 0\n",
    "    curr_split = ''\n",
    "    ctx_split = context.split()\n",
    "    whitespaces = re.findall('\\s+', context)\n",
    "    num_whitespaces = [len(token) for token in whitespaces]\n",
    "    num_whitespaces.append(1)\n",
    "    tokens = word_tokenize(context)\n",
    "    tokens = ['\"' if token not in context else token for token in tokens]\n",
    "    for index, token in enumerate(tokens, 1):\n",
    "        targets.append((token, index, curr_pos, (curr_pos + len(token))))\n",
    "        curr_pos += len(token)\n",
    "        curr_split += token\n",
    "        if ctx_split[j] == curr_split:\n",
    "            curr_pos += num_whitespaces[w]\n",
    "            j += 1\n",
    "            w += 1\n",
    "            curr_split = ''\n",
    "    return [val for val in targets if val[0] != '\"']\n",
    "\n",
    "def build_vocabulary(sentences, embedding_model, dimension):\n",
    "    all_words = [tpl[0] for sentence in sentences for tpl in sentence['seq']] + list(wordlist_lowercased)\n",
    "    print('# Words : {}'.format(len(all_words)))\n",
    "    counter = Counter(all_words)\n",
    "    vocab_size = len(counter) + 1\n",
    "    print('# Vocab : {}'.format(vocab_size))\n",
    "    print('# embeding model  : {}'.format(len(embedding_model)))   \n",
    "    word2index = {word : index for index, (word, count) in enumerate(counter.most_common(), 1)}\n",
    "    index2word = {index : word for word, index in word2index.items()}\n",
    "    # +1 required for pad token\n",
    "    embedding_matrix = np.zeros(((vocab_size), dimension))\n",
    "    missing_embed_words = []\n",
    "    i_ = 0\n",
    "    for word, index in word2index.items():\n",
    "        if word in embedding_model.key_to_index:\n",
    "            embedding = embedding_model[word]\n",
    "        else:\n",
    "             i_ +=1\n",
    "             missing_embed_words.append(word)\n",
    "             continue\n",
    "        embedding_matrix[index] = embedding\n",
    "    missing_embed_count = len(missing_embed_words)\n",
    "    print('# Words missing embedding : {}'.format(missing_embed_count))\n",
    "    print('Embedding shape : {}'.format(embedding_matrix.shape))\n",
    "    print(\"i: \", i_ )\n",
    "    return word2index, index2word, embedding_matrix\n",
    "\n",
    "def forward_transformation(dataframe, lowercase = True, filter_punc = True, filtering = \"a132\"):\n",
    "    grouped = dataframe.groupby('sentence').apply(lambda row : \n",
    "                        {'sent_id' : list(set(row['sent_id']))[0],\n",
    "                         'sentence' : list(set(row['sentence']))[0], \n",
    "                         'tags': [tag for tag in zip(row['target'], \n",
    "                            row['start'], row['end'], row['binary'], row['prob'])]})\n",
    "    sentences = []\n",
    "    for vals in grouped:\n",
    "        sent_id = vals['sent_id']\n",
    "        sentence = vals['sentence']\n",
    "        tags = vals['tags']\n",
    "        tags_without_labels = [(word, start, end) for word, start, end, binary, prob in tags]\n",
    "        all_tokens = all_tokens_with_index(sentence)\n",
    "        sent_repr = [(word, start, end, tags[tags_without_labels.index((word, start, end))][3],\n",
    "                     tags[tags_without_labels.index((word, start, end))][4])\n",
    "           if (word, start, end) in tags_without_labels \n",
    "          else (word, start, end, 0, 0.0) for word, index, start, end in all_tokens]\n",
    "        if lowercase:\n",
    "            sent_repr = [(word.lower(), start, end, binary, prob) \n",
    "                         for word, start, end, binary, prob in sent_repr]\n",
    "        if filter_punc:\n",
    "            sent_repr = list(filter(lambda vals : remove_punctuation(vals[0]), sent_repr))\n",
    "        if filtering:\n",
    "            sent_repr = list(filter(lambda vals : vals[0] != \"'s\", sent_repr))\n",
    "            sent_repr = list(filter(lambda vals : vals[0] != \"``\", sent_repr))\n",
    "        sentences.append({'sent_id' : sent_id, 'sentence' : sentence, 'seq' : sent_repr})\n",
    "    return sentences\n",
    "\n",
    "def split_sentence_seqs(sentences):\n",
    "    words, start_end, binary, prob = [], [], [] ,[]\n",
    "    for sent in sentences:\n",
    "        sequence = sent['seq']\n",
    "        curr_w, curr_se, curr_b, curr_p = map(list, zip(*[(vals[0], \n",
    "            (vals[1], vals[2]), vals[3], vals[4]) for vals in sequence]))\n",
    "        words.append(curr_w)\n",
    "        start_end.append(curr_se)\n",
    "        binary.append(curr_b)\n",
    "        prob.append(curr_p)\n",
    "    return words, start_end, binary, prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = forward_transformation(dataset)\n",
    "train_sentences = sentences[:train_num_sents]\n",
    "test_sentences = sentences[train_num_sents:]\n",
    "words, start_end, binary, prob = split_sentence_seqs(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = glove_models[0].model\n",
    "dimension = embedding_model.vector_size\n",
    "word2index, index2word, embedding = build_vocabulary(sentences, embedding_model, dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "words_with_indices = [[word2index[word] for word in sent] for sent in words]\n",
    "sent_lens = [len(sentence['seq']) for sentence in sentences]\n",
    "sent_max_length = np.max(sent_lens)\n",
    "print('Max length sentence : {}'.format(sent_max_length))\n",
    "\n",
    "\n",
    "words_padded = pad_sequences(maxlen=sent_max_length, sequences=words_with_indices, padding=\"post\", value=0)\n",
    "binary_padded = pad_sequences(maxlen=sent_max_length, sequences=binary, padding=\"post\", value=0)\n",
    "prob_padded = pad_sequences(maxlen=sent_max_length, sequences=prob, padding=\"post\", value=0, dtype=\"float\")\n",
    "\n",
    "binary_padded_categorical = [to_categorical(clazz, num_classes=2) for clazz in binary_padded]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_with_indices = [[word2index[word] for word in sent] for sent in words]\n",
    "sent_lens = [len(sentence['seq']) for sentence in sentences]\n",
    "sent_max_length = np.max(sent_lens)\n",
    "print('Max length sentence : {}'.format(sent_max_length))\n",
    "\n",
    "words_padded = pad_sequences(maxlen=sent_max_length, sequences=words_with_indices, padding=\"post\", value=0)\n",
    "binary_padded = pad_sequences(maxlen=sent_max_length, sequences=binary, padding=\"post\", value=0)\n",
    "prob_padded = pad_sequences(maxlen=sent_max_length, sequences=prob, padding=\"post\", value=0, dtype=\"float\")\n",
    "\n",
    "binary_padded_categorical = [to_categorical(clazz, num_classes=2) for clazz in binary_padded]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) Training set\n",
    "train_words_padded = words_padded[:train_num_sents]\n",
    "train_binary_padded = binary_padded[:train_num_sents]\n",
    "train_binary_padded_categorical = binary_padded_categorical[:train_num_sents]\n",
    "train_prob_padded = prob_padded[:train_num_sents]\n",
    "train_start_end = start_end[:train_num_sents]\n",
    "\n",
    "# (2) Test set\n",
    "test_words_padded = words_padded[train_num_sents:]\n",
    "test_binary_padded = binary_padded[train_num_sents:]\n",
    "test_binary_padded_categorical = binary_padded_categorical[train_num_sents:]\n",
    "test_prob_padded = prob_padded[train_num_sents:]\n",
    "test_start_end = start_end[train_num_sents:]\n",
    "\n",
    "print('Training set length : {}'.format(len(train_words_padded)))\n",
    "print('Test set length : {}'.format(len(test_words_padded)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics(tensorflow.keras.callbacks.Callback):\n",
    "    def __init__(self, validation_data):\n",
    "        self.f1_scores = []\n",
    "        self.validation_data = validation_data\n",
    "        \n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        predict = np.asarray(self.model.predict(self.validation_data[0]))\n",
    "        targ = self.validation_data[1]\n",
    "        targ = np.array(targ)\n",
    "        shape = targ.shape\n",
    "        targ = targ.reshape((shape[0]*shape[1], shape[2]))\n",
    "        targ = np.argmax(targ, axis = 1)\n",
    "        predict = predict.reshape((shape[0]*shape[1]), shape[2])\n",
    "        predict = np.argmax(predict, axis = 1)\n",
    "        self.f1s=f1_score(targ, predict)\n",
    "        print(\"\\nF1 Score:\")\n",
    "        print(f1_score(targ, np.ones(shape[0]*shape[1])))\n",
    "        self.f1_scores.append(self.f1s)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_words_padded.shape, np.array(train_binary_padded_categorical).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding.shape, sent_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = embedding.shape[0]\n",
    "dimension = embedding.shape[1]\n",
    "\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "\n",
    "in_seq = Input(shape=(sent_max_length,))\n",
    "embed = Embedding(input_dim=vocab_size, output_dim=dimension, \\\n",
    "                  weights=[embedding], input_length=sent_max_length)(in_seq)\n",
    "drop = Dropout(0.1)(embed)\n",
    "lstm = Bidirectional(LSTM(units=150, return_sequences=True, recurrent_dropout=0.1))(drop)\n",
    "out = TimeDistributed(Dense(2, activation=\"softmax\"))(lstm) \n",
    "\n",
    "model = Model(in_seq, out)\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "model.summary()\n",
    "\n",
    "metrics = Metrics((test_words_padded, np.array(test_binary_padded_categorical)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_words_padded, np.array(train_binary_padded_categorical), batch_size=10, \n",
    "                    epochs=3, validation_data = (test_words_padded, np.array(test_binary_padded_categorical)), \n",
    "                    verbose=1, callbacks=[metrics])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"darkgrid\")\n",
    "epoch_f1s = plt.plot(metrics.f1_scores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dir = '../../../../models/lexical/bertls/model_CWI_full.h5'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(path_dir)  # creates a HDF5 file 'model_CWI_full.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cwi = load_model(path_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cwi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_ = set(stopwords.words('english'))\n",
    "def cleaner(word):\n",
    "  #Remove links\n",
    "  word = re.sub(r'((http|https)\\:\\/\\/)?[a-zA-Z0-9\\.\\/\\?\\:@\\-_=#]+\\.([a-zA-Z]){2,6}([a-zA-Z0-9\\.\\&\\/\\?\\:@\\-_=#])*', \n",
    "                '', word, flags=re.MULTILINE)\n",
    "  word = re.sub('[\\W]', ' ', word)\n",
    "  word = re.sub('[^a-zA-Z]', ' ', word)\n",
    "  return word.lower().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_input(input_text):\n",
    "  input_text = cleaner(input_text)\n",
    "  clean_text = []\n",
    "  index_list =[]\n",
    "  input_token = []\n",
    "  index_list_zipf = []\n",
    "  for i, word in enumerate(input_text.split()):\n",
    "    if word in word2index:\n",
    "      clean_text.append(word)\n",
    "      input_token.append(word2index[word])\n",
    "    else:\n",
    "      index_list.append(i)\n",
    "  input_padded = pad_sequences(maxlen=sent_max_length, sequences=[input_token], padding=\"post\", value=0)\n",
    "  return input_padded, index_list, len(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_missing_word(pred_binary, index_list, len_list):\n",
    "  list_cwi_predictions = list(pred_binary[0][:len_list])\n",
    "  for i in index_list:\n",
    "    list_cwi_predictions.insert(i, 0)\n",
    "  return list_cwi_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = 'bert-large-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model)\n",
    "model = BertForMaskedLM.from_pretrained(bert_model)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_candidates(input_text, list_cwi_predictions, numb_predictions_displayed = 10):\n",
    "  list_candidates_bert = []\n",
    "  for word,pred  in zip(input_text.split(), list_cwi_predictions):\n",
    "    if (pred and (pos_tag([word])[0][1] in ['NNS', 'NN', 'VBP', 'RB', 'VBG','VBD' ]))  or (zipf_frequency(word, 'en')) <3.1:\n",
    "      replace_word_mask = input_text.replace(word, '[MASK]')\n",
    "      text = f'[CLS]{replace_word_mask} [SEP] {input_text} [SEP] '\n",
    "      tokenized_text = tokenizer.tokenize(text)\n",
    "      masked_index = [i for i, x in enumerate(tokenized_text) if x == '[MASK]'][0]\n",
    "      indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "      segments_ids = [0]*len(tokenized_text)\n",
    "      tokens_tensor = torch.tensor([indexed_tokens])\n",
    "      segments_tensors = torch.tensor([segments_ids])\n",
    "      # Predict all tokens\n",
    "      with torch.no_grad():\n",
    "          outputs = model(tokens_tensor, token_type_ids=segments_tensors)\n",
    "          predictions = outputs[0][0][masked_index]\n",
    "      predicted_ids = torch.argsort(predictions, descending=True)[:numb_predictions_displayed]\n",
    "      predicted_tokens = tokenizer.convert_ids_to_tokens(list(predicted_ids))\n",
    "      list_candidates_bert.append((word, predicted_tokens))\n",
    "  return list_candidates_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_texts = [ \n",
    " 'The Risk That Students Could Arrive at School With the Coronavirus As schools grapple with how to reopen, new estimates show that large parts of the country would probably see infected students if classrooms opened now.',\n",
    " 'How a photograph of a young man cradling his dying friend sent me on a journey across India.',\n",
    " 'Pro-democracy parties, which had hoped to ride widespread discontent to big gains, saw the yearlong delay as an attempt to thwart them.',\n",
    " 'Night after night, calm gave way to chaos. See what happened between the protesters and the federal agents.',\n",
    " 'Contact Tracing Is Failing in Many States. Here is Why. Inadequate testing and protracted delays in producing results have crippled tracking and hampered efforts to contain major outbreaks.',\n",
    " 'After an initial decrease in the youth detention population, the rate of release has slowed, and the gap between white youth and Black youth has grown.'\n",
    " 'A laboratory experiment hints at some of the ways the virus might elude antibody treatments. Combining therapies could help, experts said.',\n",
    " 'Though I may not be here with you, I urge you to answer the highest calling of your heart and stand up for what you truly believe.',\n",
    " 'The research does not prove that infected children are contagious, but it should influence the debate about reopening schools, some experts said.',\n",
    " 'Dropping antibody counts are not a sign that our immune system is failing against the coronavirus, nor an omen that we can not develop a viable vaccine.',\n",
    " 'The Senate majority leader has said he will not approve a stimulus package without a “liability shield,” but top White House officials say they do not see it as essential.',\n",
    " 'Campaign efforts to refocus come as the president continues to push divisive messages that have frustrated his own party.'\n",
    "] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for input_text in list_texts:\n",
    "  new_text = input_text\n",
    "  input_padded, index_list, len_list = process_input(input_text)\n",
    "  pred_cwi = model_cwi.predict(input_padded)\n",
    "  pred_cwi_binary = np.argmax(pred_cwi, axis = 2)\n",
    "  complete_cwi_predictions = complete_missing_word(pred_cwi_binary, index_list, len_list)\n",
    "  bert_candidates =   get_bert_candidates(input_text, complete_cwi_predictions)\n",
    "  for word_to_replace, l_candidates in bert_candidates:\n",
    "    tuples_word_zipf = []\n",
    "    for w in l_candidates:\n",
    "      if w.isalpha():\n",
    "        tuples_word_zipf.append((w, zipf_frequency(w, 'en')))\n",
    "    tuples_word_zipf = sorted(tuples_word_zipf, key = lambda x: x[1], reverse=True)\n",
    "    new_text = re.sub(word_to_replace, tuples_word_zipf[0][0], new_text) \n",
    "  print(\"Original text: \", input_text )\n",
    "  print(\"Simplified text:\", new_text, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "254e3b970a7dd062895c9e26f37fa2538908b1e74b276da92edb09857b48a424"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit ('simplification': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
